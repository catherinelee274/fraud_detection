{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adtracking Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Kyle O'Brien, Catherine Lee, Amit Saxena "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is in \"data/\" and includes the training and testing csv files.\n",
    "Evaluation metrics will include looking at R^2 and a confusion matrix. Other things to try, CNN (Resnet, inceptionv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np         # linear algebra\n",
    "import sklearn as sk       # machine learning\n",
    "import pandas as pd        # reading in data files, data cleaning\n",
    "import matplotlib.pyplot as plt   # for plotting\n",
    "import seaborn as sns      # visualization tool\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_sample: 100,000 randomly-selected rows of training data (because the full training data takes too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83252</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:42:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106590</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:43:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147164</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>134</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39782</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>205</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121646</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel           click_time attributed_time  \\\n",
       "0   83252    3       1  16      379  2017-11-06 15:42:19             NaN   \n",
       "1  106590    3       1  25      379  2017-11-06 15:43:23             NaN   \n",
       "2  147164   14       1  28      134  2017-11-06 16:00:00             NaN   \n",
       "3   39782    2       1  10      205  2017-11-06 16:00:00             NaN   \n",
       "4  121646   23       1  13      153  2017-11-06 16:00:00             NaN   \n",
       "\n",
       "   is_attributed  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data in \n",
    "\n",
    "data = pd.read_csv(\"data/equalized_train.csv\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91473 entries, 0 to 91472\n",
      "Data columns (total 8 columns):\n",
      "ip                 91473 non-null int64\n",
      "app                91473 non-null int64\n",
      "device             91473 non-null int64\n",
      "os                 91473 non-null int64\n",
      "channel            91473 non-null int64\n",
      "click_time         91473 non-null object\n",
      "attributed_time    41473 non-null object\n",
      "is_attributed      91473 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping attributed_time because many null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate click_time into multiple columns\n",
    "#data['click_time'].split(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.drop(columns=['attributed_time'])\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    91473.000000\n",
       "mean         0.453391\n",
       "std          0.497826\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: is_attributed, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attributed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_attributed\n",
      "0    50000\n",
      "1    41473\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#plotx = data['attributed_time']\n",
    "#ploty = data['is_attributed']\n",
    "\n",
    "#plt.scatter(plotx,ploty)\n",
    "import pandas\n",
    "class_counts = data.groupby('is_attributed').size()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that we have a major imbalance in our data. This is incentivising our model to guess 0 for overthing and it would still receive a 99% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['app', 'device', 'os', 'channel']]\n",
    "y = data['is_attributed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22243</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58933</th>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50574</th>\n",
       "      <td>18</td>\n",
       "      <td>3032</td>\n",
       "      <td>607</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63812</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52847</th>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       app  device   os  channel\n",
       "22243    2       1   20      477\n",
       "58933  105       1   17      451\n",
       "50574   18    3032  607      107\n",
       "63812   11       1   13      325\n",
       "52847  116       1   41      101"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22243    0\n",
       "58933    1\n",
       "50574    0\n",
       "63812    1\n",
       "52847    1\n",
       "Name: is_attributed, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems a little strange that we would get such high accuracy. This might have to do with the fact that we are using R^2 to measure the goodness of our model. There might be an imbalance in the test set of which clicks are 0 or 1, so we should try precision and recall. (Longterm with full training data, try to keep an equal number of classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy 0.6349822355834928\n"
     ]
    }
   ],
   "source": [
    "logreg_score = logreg.score(X_test, y_test)\n",
    "print(\"Logistic Regression Accuracy\", logreg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy: 0.9352282044274391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier()\n",
    "RandomForest.fit(X_train,y_train)\n",
    "rf_score = RandomForest.score(X_test,y_test)\n",
    "print(\"Random Forest accuracy:\", rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94      9971\n",
      "           1       0.96      0.89      0.93      8324\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     18295\n",
      "   macro avg       0.94      0.93      0.93     18295\n",
      "weighted avg       0.94      0.94      0.93     18295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall Score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = RandomForest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lSVC = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: 0.7056572834107679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lSVC.fit(X_train,y_train)\n",
    "svm_score = lSVC.score(X_test,y_test)\n",
    "print(\"SVM accuracy:\", svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.93517354468434\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "\n",
    "dtc = dtc.fit(X_train,y_train)\n",
    "dtc_score =  dtc.score(X_test,y_test)\n",
    "print(\"Decision Tree Accuracy:\", dtc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      9971\n",
      "           1       0.96      0.89      0.93      8324\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     18295\n",
      "   macro avg       0.94      0.93      0.93     18295\n",
      "weighted avg       0.94      0.94      0.93     18295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall Score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9690  281]\n",
      " [ 905 7419]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "#In Progress\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "#plt.title(\"Confusion Matrix\")\n",
    "\n",
    "#plt.colorbar()\n",
    "#tick_marks = np.arange(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "knn_score = knn.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, y_train)\n",
    "Y_pred = gaussian.predict(X_test)\n",
    "gaussian_score = gaussian.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train)\n",
    "Y_pred = sgd.predict(X_test)\n",
    "sgd_score = sgd.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "Y_pred = perceptron.predict(X_test)\n",
    "perc_score = perceptron.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "XGB = xgb.XGBClassifier()\n",
    "XGB.fit(X_train, y_train)\n",
    "Y_pred = XGB.predict(X_test)\n",
    "XGB_score = XGB.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which model do we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.935228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.935174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K Nearest Neighbors</td>\n",
       "      <td>0.921235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.705657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.705657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.688385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.685051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.634982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.612845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model     Score\n",
       "2                Random Forest  0.935228\n",
       "4                Decision Tree  0.935174\n",
       "5          K Nearest Neighbors  0.921235\n",
       "0      Support Vector Machines  0.705657\n",
       "3                   Linear SVC  0.705657\n",
       "6         Gaussian Naive Bayes  0.688385\n",
       "7  Stochastic Gradient Descent  0.685051\n",
       "1          Logistic Regression  0.634982\n",
       "8                   Perceptron  0.612845"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'Logistic Regression', \n",
    "              'Random Forest', 'Linear SVC', \n",
    "              'Decision Tree', 'K Nearest Neighbors', \n",
    "              'Gaussian Naive Bayes', 'Stochastic Gradient Descent', \n",
    "              'Perceptron'],\n",
    "    'Score': [svm_score, logreg_score, rf_score, svm_score, dtc_score, \n",
    "              knn_score, gaussian_score, sgd_score, perc_score]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49029 samples, validate on 24149 samples\n",
      "Epoch 1/300\n",
      "49029/49029 [==============================] - 4s 91us/step - loss: 0.5191 - acc: 0.7534 - val_loss: 0.4505 - val_acc: 0.8090\n",
      "Epoch 2/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.4160 - acc: 0.8412 - val_loss: 0.3846 - val_acc: 0.8580\n",
      "Epoch 3/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.3772 - acc: 0.8650 - val_loss: 0.3601 - val_acc: 0.8699\n",
      "Epoch 4/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.3597 - acc: 0.8666 - val_loss: 0.3451 - val_acc: 0.8718\n",
      "Epoch 5/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.3475 - acc: 0.8712 - val_loss: 0.3358 - val_acc: 0.8743\n",
      "Epoch 6/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.3415 - acc: 0.8748 - val_loss: 0.3331 - val_acc: 0.8776\n",
      "Epoch 7/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.3348 - acc: 0.8778 - val_loss: 0.3236 - val_acc: 0.8821\n",
      "Epoch 8/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.3325 - acc: 0.8790 - val_loss: 0.3183 - val_acc: 0.8857\n",
      "Epoch 9/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.3286 - acc: 0.8823 - val_loss: 0.3224 - val_acc: 0.8869\n",
      "Epoch 10/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.3249 - acc: 0.8838 - val_loss: 0.3185 - val_acc: 0.8861\n",
      "Epoch 11/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.3227 - acc: 0.8845 - val_loss: 0.3207 - val_acc: 0.8830\n",
      "Epoch 12/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.3215 - acc: 0.8860 - val_loss: 0.3097 - val_acc: 0.8938\n",
      "Epoch 13/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.3172 - acc: 0.8887 - val_loss: 0.3115 - val_acc: 0.8934cc: 0\n",
      "Epoch 14/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.3135 - acc: 0.8913 - val_loss: 0.3029 - val_acc: 0.9004\n",
      "Epoch 15/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.3083 - acc: 0.8946 - val_loss: 0.2974 - val_acc: 0.9044\n",
      "Epoch 16/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.3058 - acc: 0.8977 - val_loss: 0.2962 - val_acc: 0.9010\n",
      "Epoch 17/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.3027 - acc: 0.8985 - val_loss: 0.2953 - val_acc: 0.9028\n",
      "Epoch 18/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.3008 - acc: 0.8992 - val_loss: 0.3000 - val_acc: 0.8961\n",
      "Epoch 19/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.2959 - acc: 0.9003 - val_loss: 0.2856 - val_acc: 0.9056\n",
      "Epoch 20/300\n",
      "49029/49029 [==============================] - 5s 92us/step - loss: 0.2918 - acc: 0.9021 - val_loss: 0.2858 - val_acc: 0.9046\n",
      "Epoch 21/300\n",
      "49029/49029 [==============================] - 5s 99us/step - loss: 0.2890 - acc: 0.9017 - val_loss: 0.2819 - val_acc: 0.9067\n",
      "Epoch 22/300\n",
      "49029/49029 [==============================] - 4s 88us/step - loss: 0.2868 - acc: 0.9024 - val_loss: 0.2791 - val_acc: 0.9076\n",
      "Epoch 23/300\n",
      "49029/49029 [==============================] - 4s 90us/step - loss: 0.2860 - acc: 0.9032 - val_loss: 0.2770 - val_acc: 0.9077\n",
      "Epoch 24/300\n",
      "49029/49029 [==============================] - 4s 86us/step - loss: 0.2847 - acc: 0.9035 - val_loss: 0.2812 - val_acc: 0.9074\n",
      "Epoch 25/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.2836 - acc: 0.9044 - val_loss: 0.2733 - val_acc: 0.9081\n",
      "Epoch 26/300\n",
      "49029/49029 [==============================] - 4s 87us/step - loss: 0.2829 - acc: 0.9043 - val_loss: 0.2745 - val_acc: 0.9073\n",
      "Epoch 27/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2812 - acc: 0.9043 - val_loss: 0.2706 - val_acc: 0.9095\n",
      "Epoch 28/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2805 - acc: 0.9056 - val_loss: 0.2728 - val_acc: 0.9078\n",
      "Epoch 29/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2795 - acc: 0.9053 - val_loss: 0.2700 - val_acc: 0.9087\n",
      "Epoch 30/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2781 - acc: 0.9055 - val_loss: 0.2707 - val_acc: 0.9099\n",
      "Epoch 31/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2776 - acc: 0.9054 - val_loss: 0.2679 - val_acc: 0.9066\n",
      "Epoch 32/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2768 - acc: 0.9054 - val_loss: 0.2690 - val_acc: 0.9062\n",
      "Epoch 33/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2755 - acc: 0.9061 - val_loss: 0.2640 - val_acc: 0.9072\n",
      "Epoch 34/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2738 - acc: 0.9066 - val_loss: 0.2686 - val_acc: 0.9084\n",
      "Epoch 35/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2721 - acc: 0.9067 - val_loss: 0.2751 - val_acc: 0.9045\n",
      "Epoch 36/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2720 - acc: 0.9056 - val_loss: 0.2681 - val_acc: 0.9115\n",
      "Epoch 37/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2711 - acc: 0.9056 - val_loss: 0.2728 - val_acc: 0.9085\n",
      "Epoch 38/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2706 - acc: 0.9063 - val_loss: 0.2616 - val_acc: 0.9104\n",
      "Epoch 39/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.2696 - acc: 0.9068 - val_loss: 0.2604 - val_acc: 0.9101\n",
      "Epoch 40/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2682 - acc: 0.9064 - val_loss: 0.2604 - val_acc: 0.9111\n",
      "Epoch 41/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2666 - acc: 0.9069 - val_loss: 0.2564 - val_acc: 0.9075\n",
      "Epoch 42/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2678 - acc: 0.9062 - val_loss: 0.2604 - val_acc: 0.9123\n",
      "Epoch 43/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2663 - acc: 0.9068 - val_loss: 0.2711 - val_acc: 0.9036\n",
      "Epoch 44/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2657 - acc: 0.9067 - val_loss: 0.2717 - val_acc: 0.9090\n",
      "Epoch 45/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2655 - acc: 0.9067 - val_loss: 0.2667 - val_acc: 0.9057\n",
      "Epoch 46/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2677 - acc: 0.9065 - val_loss: 0.2547 - val_acc: 0.9106\n",
      "Epoch 47/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2637 - acc: 0.9065 - val_loss: 0.2678 - val_acc: 0.9067\n",
      "Epoch 48/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2636 - acc: 0.9070 - val_loss: 0.2560 - val_acc: 0.9121\n",
      "Epoch 49/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.2633 - acc: 0.9079 - val_loss: 0.2589 - val_acc: 0.9069\n",
      "Epoch 50/300\n",
      "49029/49029 [==============================] - 5s 98us/step - loss: 0.2621 - acc: 0.9075 - val_loss: 0.2556 - val_acc: 0.9076\n",
      "Epoch 51/300\n",
      "49029/49029 [==============================] - 5s 92us/step - loss: 0.2625 - acc: 0.9071 - val_loss: 0.2701 - val_acc: 0.8916\n",
      "Epoch 52/300\n",
      "49029/49029 [==============================] - 4s 90us/step - loss: 0.2619 - acc: 0.9078 - val_loss: 0.2545 - val_acc: 0.9111\n",
      "Epoch 53/300\n",
      "49029/49029 [==============================] - 5s 109us/step - loss: 0.2621 - acc: 0.9073 - val_loss: 0.2546 - val_acc: 0.9104\n",
      "Epoch 54/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2615 - acc: 0.9076 - val_loss: 0.2581 - val_acc: 0.9088\n",
      "Epoch 55/300\n",
      "49029/49029 [==============================] - 5s 98us/step - loss: 0.2609 - acc: 0.9078 - val_loss: 0.2531 - val_acc: 0.9108\n",
      "Epoch 56/300\n",
      "49029/49029 [==============================] - 4s 90us/step - loss: 0.2599 - acc: 0.9076 - val_loss: 0.2613 - val_acc: 0.9105\n",
      "Epoch 57/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2606 - acc: 0.9074 - val_loss: 0.2521 - val_acc: 0.9110\n",
      "Epoch 58/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2595 - acc: 0.9081 - val_loss: 0.2597 - val_acc: 0.9084\n",
      "Epoch 59/300\n",
      "49029/49029 [==============================] - 4s 91us/step - loss: 0.2589 - acc: 0.9074 - val_loss: 0.2546 - val_acc: 0.9113\n",
      "Epoch 60/300\n",
      "49029/49029 [==============================] - 4s 87us/step - loss: 0.2593 - acc: 0.9084 - val_loss: 0.2636 - val_acc: 0.9019\n",
      "Epoch 61/300\n",
      "49029/49029 [==============================] - 6s 129us/step - loss: 0.2589 - acc: 0.9072 - val_loss: 0.2572 - val_acc: 0.9094\n",
      "Epoch 62/300\n",
      "49029/49029 [==============================] - 5s 107us/step - loss: 0.2577 - acc: 0.9084 - val_loss: 0.2547 - val_acc: 0.9113\n",
      "Epoch 63/300\n",
      "49029/49029 [==============================] - 6s 117us/step - loss: 0.2588 - acc: 0.9083 - val_loss: 0.2547 - val_acc: 0.9105\n",
      "Epoch 64/300\n",
      "49029/49029 [==============================] - 5s 96us/step - loss: 0.2580 - acc: 0.9076 - val_loss: 0.2572 - val_acc: 0.9086\n",
      "Epoch 65/300\n",
      "49029/49029 [==============================] - 6s 129us/step - loss: 0.2577 - acc: 0.9081 - val_loss: 0.2773 - val_acc: 0.9042\n",
      "Epoch 66/300\n",
      "49029/49029 [==============================] - 5s 107us/step - loss: 0.2574 - acc: 0.9081 - val_loss: 0.2504 - val_acc: 0.9094\n",
      "Epoch 67/300\n",
      "49029/49029 [==============================] - 4s 88us/step - loss: 0.2581 - acc: 0.9074 - val_loss: 0.2514 - val_acc: 0.9092\n",
      "Epoch 68/300\n",
      "49029/49029 [==============================] - 5s 95us/step - loss: 0.2565 - acc: 0.9083 - val_loss: 0.2501 - val_acc: 0.9082\n",
      "Epoch 69/300\n",
      "49029/49029 [==============================] - 5s 103us/step - loss: 0.2563 - acc: 0.9079 - val_loss: 0.2519 - val_acc: 0.9132\n",
      "Epoch 70/300\n",
      "49029/49029 [==============================] - 4s 85us/step - loss: 0.2566 - acc: 0.9078 - val_loss: 0.2485 - val_acc: 0.9080\n",
      "Epoch 71/300\n",
      "49029/49029 [==============================] - 4s 88us/step - loss: 0.2554 - acc: 0.9086 - val_loss: 0.2527 - val_acc: 0.9116\n",
      "Epoch 72/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.2557 - acc: 0.9086 - val_loss: 0.2528 - val_acc: 0.9110\n",
      "Epoch 73/300\n",
      "49029/49029 [==============================] - 5s 102us/step - loss: 0.2549 - acc: 0.9084 - val_loss: 0.2519 - val_acc: 0.9091\n",
      "Epoch 74/300\n",
      "49029/49029 [==============================] - 5s 100us/step - loss: 0.2556 - acc: 0.9082 - val_loss: 0.2499 - val_acc: 0.9115\n",
      "Epoch 75/300\n",
      "49029/49029 [==============================] - 5s 107us/step - loss: 0.2555 - acc: 0.9083 - val_loss: 0.2502 - val_acc: 0.9099\n",
      "Epoch 76/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.2545 - acc: 0.9088 - val_loss: 0.2487 - val_acc: 0.9113\n",
      "Epoch 77/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.2548 - acc: 0.9088 - val_loss: 0.2468 - val_acc: 0.9112\n",
      "Epoch 78/300\n",
      "49029/49029 [==============================] - 5s 101us/step - loss: 0.2542 - acc: 0.9087 - val_loss: 0.2460 - val_acc: 0.9109\n",
      "Epoch 79/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2538 - acc: 0.9085 - val_loss: 0.2511 - val_acc: 0.9105\n",
      "Epoch 80/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2540 - acc: 0.9091 - val_loss: 0.2626 - val_acc: 0.8913\n",
      "Epoch 81/300\n",
      "49029/49029 [==============================] - 6s 127us/step - loss: 0.2534 - acc: 0.9082 - val_loss: 0.2649 - val_acc: 0.8984\n",
      "Epoch 82/300\n",
      "49029/49029 [==============================] - 6s 130us/step - loss: 0.2538 - acc: 0.9090 - val_loss: 0.2505 - val_acc: 0.9091\n",
      "Epoch 83/300\n",
      "49029/49029 [==============================] - 5s 110us/step - loss: 0.2533 - acc: 0.9094 - val_loss: 0.2521 - val_acc: 0.9087\n",
      "Epoch 84/300\n",
      "49029/49029 [==============================] - 5s 99us/step - loss: 0.2525 - acc: 0.9094 - val_loss: 0.2500 - val_acc: 0.9084\n",
      "Epoch 85/300\n",
      "49029/49029 [==============================] - 5s 110us/step - loss: 0.2531 - acc: 0.9091 - val_loss: 0.2578 - val_acc: 0.9071\n",
      "Epoch 86/300\n",
      "49029/49029 [==============================] - 5s 100us/step - loss: 0.2526 - acc: 0.9093 - val_loss: 0.2525 - val_acc: 0.9074\n",
      "Epoch 87/300\n",
      "49029/49029 [==============================] - 4s 87us/step - loss: 0.2528 - acc: 0.9089 - val_loss: 0.2591 - val_acc: 0.9048\n",
      "Epoch 88/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2524 - acc: 0.9098 - val_loss: 0.2471 - val_acc: 0.9136\n",
      "Epoch 89/300\n",
      "49029/49029 [==============================] - 5s 95us/step - loss: 0.2521 - acc: 0.9092 - val_loss: 0.2475 - val_acc: 0.9090\n",
      "Epoch 90/300\n",
      "49029/49029 [==============================] - 4s 90us/step - loss: 0.2519 - acc: 0.9092 - val_loss: 0.2486 - val_acc: 0.9138\n",
      "Epoch 91/300\n",
      "49029/49029 [==============================] - 6s 129us/step - loss: 0.2518 - acc: 0.9094 - val_loss: 0.2508 - val_acc: 0.9104\n",
      "Epoch 92/300\n",
      "49029/49029 [==============================] - 6s 114us/step - loss: 0.2526 - acc: 0.9088 - val_loss: 0.2465 - val_acc: 0.9108\n",
      "Epoch 93/300\n",
      "49029/49029 [==============================] - 7s 137us/step - loss: 0.2527 - acc: 0.9096 - val_loss: 0.2576 - val_acc: 0.9107\n",
      "Epoch 94/300\n",
      "49029/49029 [==============================] - 4s 86us/step - loss: 0.2513 - acc: 0.9095 - val_loss: 0.2432 - val_acc: 0.9117\n",
      "Epoch 95/300\n",
      "49029/49029 [==============================] - 6s 113us/step - loss: 0.2498 - acc: 0.9096 - val_loss: 0.2503 - val_acc: 0.9121\n",
      "Epoch 96/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2506 - acc: 0.9105 - val_loss: 0.2437 - val_acc: 0.9129\n",
      "Epoch 97/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.2508 - acc: 0.9106 - val_loss: 0.2465 - val_acc: 0.9103\n",
      "Epoch 98/300\n",
      "49029/49029 [==============================] - 5s 92us/step - loss: 0.2514 - acc: 0.9095 - val_loss: 0.2455 - val_acc: 0.9118\n",
      "Epoch 99/300\n",
      "49029/49029 [==============================] - 5s 105us/step - loss: 0.2507 - acc: 0.9103 - val_loss: 0.2583 - val_acc: 0.9123\n",
      "Epoch 100/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2502 - acc: 0.9103 - val_loss: 0.2443 - val_acc: 0.9113\n",
      "Epoch 101/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.2504 - acc: 0.9107 - val_loss: 0.2586 - val_acc: 0.9061\n",
      "Epoch 102/300\n",
      "49029/49029 [==============================] - 5s 95us/step - loss: 0.2501 - acc: 0.9100 - val_loss: 0.2422 - val_acc: 0.9130\n",
      "Epoch 103/300\n",
      "49029/49029 [==============================] - 6s 114us/step - loss: 0.2498 - acc: 0.9105 - val_loss: 0.2457 - val_acc: 0.9121\n",
      "Epoch 104/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2515 - acc: 0.9093 - val_loss: 0.2437 - val_acc: 0.9104\n",
      "Epoch 105/300\n",
      "49029/49029 [==============================] - 3s 67us/step - loss: 0.2496 - acc: 0.9102 - val_loss: 0.2423 - val_acc: 0.9139\n",
      "Epoch 106/300\n",
      "49029/49029 [==============================] - 4s 86us/step - loss: 0.2501 - acc: 0.9098 - val_loss: 0.2605 - val_acc: 0.8986\n",
      "Epoch 107/300\n",
      "49029/49029 [==============================] - 5s 94us/step - loss: 0.2491 - acc: 0.9110 - val_loss: 0.2458 - val_acc: 0.9138\n",
      "Epoch 108/300\n",
      "49029/49029 [==============================] - 6s 118us/step - loss: 0.2490 - acc: 0.9108 - val_loss: 0.2540 - val_acc: 0.9108\n",
      "Epoch 109/300\n",
      "49029/49029 [==============================] - 5s 103us/step - loss: 0.2495 - acc: 0.9104 - val_loss: 0.2454 - val_acc: 0.9128\n",
      "Epoch 110/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.2487 - acc: 0.9107 - val_loss: 0.2498 - val_acc: 0.9115\n",
      "Epoch 111/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.2498 - acc: 0.9107 - val_loss: 0.2411 - val_acc: 0.9141\n",
      "Epoch 112/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2490 - acc: 0.9104 - val_loss: 0.2431 - val_acc: 0.9123\n",
      "Epoch 113/300\n",
      "49029/49029 [==============================] - 3s 67us/step - loss: 0.2488 - acc: 0.9113 - val_loss: 0.2450 - val_acc: 0.9123\n",
      "Epoch 114/300\n",
      "49029/49029 [==============================] - 4s 81us/step - loss: 0.2497 - acc: 0.9105 - val_loss: 0.2409 - val_acc: 0.9139\n",
      "Epoch 115/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.2493 - acc: 0.9100 - val_loss: 0.2496 - val_acc: 0.9048\n",
      "Epoch 116/300\n",
      "49029/49029 [==============================] - 3s 66us/step - loss: 0.2483 - acc: 0.9105 - val_loss: 0.2476 - val_acc: 0.9114\n",
      "Epoch 117/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2484 - acc: 0.9110 - val_loss: 0.2491 - val_acc: 0.9059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2488 - acc: 0.9107 - val_loss: 0.2475 - val_acc: 0.9125\n",
      "Epoch 119/300\n",
      "49029/49029 [==============================] - 5s 92us/step - loss: 0.2483 - acc: 0.9107 - val_loss: 0.2445 - val_acc: 0.9139\n",
      "Epoch 120/300\n",
      "49029/49029 [==============================] - 5s 98us/step - loss: 0.2483 - acc: 0.9107 - val_loss: 0.2417 - val_acc: 0.9117\n",
      "Epoch 121/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2480 - acc: 0.9114 - val_loss: 0.2478 - val_acc: 0.9109\n",
      "Epoch 122/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2481 - acc: 0.9108 - val_loss: 0.2420 - val_acc: 0.9143\n",
      "Epoch 123/300\n",
      "49029/49029 [==============================] - 4s 91us/step - loss: 0.2471 - acc: 0.9114 - val_loss: 0.2416 - val_acc: 0.9113\n",
      "Epoch 124/300\n",
      "49029/49029 [==============================] - 5s 107us/step - loss: 0.2464 - acc: 0.9114 - val_loss: 0.2447 - val_acc: 0.9133\n",
      "Epoch 125/300\n",
      "49029/49029 [==============================] - 5s 95us/step - loss: 0.2484 - acc: 0.9107 - val_loss: 0.2410 - val_acc: 0.9136\n",
      "Epoch 126/300\n",
      "49029/49029 [==============================] - 4s 81us/step - loss: 0.2475 - acc: 0.9113 - val_loss: 0.2419 - val_acc: 0.9137\n",
      "Epoch 127/300\n",
      "49029/49029 [==============================] - 4s 81us/step - loss: 0.2475 - acc: 0.9110 - val_loss: 0.2449 - val_acc: 0.90570s - loss: 0.2479\n",
      "Epoch 128/300\n",
      "49029/49029 [==============================] - 4s 87us/step - loss: 0.2465 - acc: 0.9114 - val_loss: 0.2443 - val_acc: 0.9140\n",
      "Epoch 129/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.2471 - acc: 0.9112 - val_loss: 0.2385 - val_acc: 0.9140\n",
      "Epoch 130/300\n",
      "49029/49029 [==============================] - 5s 102us/step - loss: 0.2467 - acc: 0.9114 - val_loss: 0.2432 - val_acc: 0.9140\n",
      "Epoch 131/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.2470 - acc: 0.9116 - val_loss: 0.2419 - val_acc: 0.9135\n",
      "Epoch 132/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.2462 - acc: 0.9111 - val_loss: 0.2420 - val_acc: 0.9135\n",
      "Epoch 133/300\n",
      "49029/49029 [==============================] - 4s 87us/step - loss: 0.2466 - acc: 0.9116 - val_loss: 0.2469 - val_acc: 0.9111\n",
      "Epoch 134/300\n",
      "49029/49029 [==============================] - 5s 96us/step - loss: 0.2462 - acc: 0.9109 - val_loss: 0.2399 - val_acc: 0.9141\n",
      "Epoch 135/300\n",
      "49029/49029 [==============================] - 6s 117us/step - loss: 0.2460 - acc: 0.9111 - val_loss: 0.2397 - val_acc: 0.9140\n",
      "Epoch 136/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2473 - acc: 0.9115 - val_loss: 0.2426 - val_acc: 0.9137\n",
      "Epoch 137/300\n",
      "49029/49029 [==============================] - 4s 81us/step - loss: 0.2464 - acc: 0.9115 - val_loss: 0.2438 - val_acc: 0.9143\n",
      "Epoch 138/300\n",
      "49029/49029 [==============================] - 5s 101us/step - loss: 0.2457 - acc: 0.9123 - val_loss: 0.2408 - val_acc: 0.9143\n",
      "Epoch 139/300\n",
      "49029/49029 [==============================] - 4s 91us/step - loss: 0.2455 - acc: 0.9116 - val_loss: 0.2483 - val_acc: 0.9128\n",
      "Epoch 140/300\n",
      "49029/49029 [==============================] - 5s 108us/step - loss: 0.2458 - acc: 0.9112 - val_loss: 0.2512 - val_acc: 0.9090\n",
      "Epoch 141/300\n",
      "49029/49029 [==============================] - 5s 108us/step - loss: 0.2459 - acc: 0.9112 - val_loss: 0.2396 - val_acc: 0.9145\n",
      "Epoch 142/300\n",
      "49029/49029 [==============================] - 5s 94us/step - loss: 0.2456 - acc: 0.9120 - val_loss: 0.2441 - val_acc: 0.9127\n",
      "Epoch 143/300\n",
      "49029/49029 [==============================] - 5s 109us/step - loss: 0.2456 - acc: 0.9117 - val_loss: 0.2404 - val_acc: 0.9145\n",
      "Epoch 144/300\n",
      "49029/49029 [==============================] - 5s 107us/step - loss: 0.2460 - acc: 0.9117 - val_loss: 0.2422 - val_acc: 0.9134\n",
      "Epoch 145/300\n",
      "49029/49029 [==============================] - 6s 114us/step - loss: 0.2458 - acc: 0.9116 - val_loss: 0.2398 - val_acc: 0.9127\n",
      "Epoch 146/300\n",
      "49029/49029 [==============================] - 5s 103us/step - loss: 0.2446 - acc: 0.9120 - val_loss: 0.2406 - val_acc: 0.9134\n",
      "Epoch 147/300\n",
      "49029/49029 [==============================] - 5s 97us/step - loss: 0.2450 - acc: 0.9121 - val_loss: 0.2426 - val_acc: 0.9141\n",
      "Epoch 148/300\n",
      "49029/49029 [==============================] - 5s 101us/step - loss: 0.2453 - acc: 0.9116 - val_loss: 0.2378 - val_acc: 0.9141\n",
      "Epoch 149/300\n",
      "49029/49029 [==============================] - 5s 104us/step - loss: 0.2450 - acc: 0.9116 - val_loss: 0.2397 - val_acc: 0.9134\n",
      "Epoch 150/300\n",
      "49029/49029 [==============================] - 4s 88us/step - loss: 0.2445 - acc: 0.9115 - val_loss: 0.2421 - val_acc: 0.9121\n",
      "Epoch 151/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.2445 - acc: 0.9122 - val_loss: 0.2419 - val_acc: 0.9128\n",
      "Epoch 152/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.2443 - acc: 0.9121 - val_loss: 0.2378 - val_acc: 0.9143\n",
      "Epoch 153/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2441 - acc: 0.9122 - val_loss: 0.2457 - val_acc: 0.9119\n",
      "Epoch 154/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2442 - acc: 0.9123 - val_loss: 0.2452 - val_acc: 0.9123\n",
      "Epoch 155/300\n",
      "49029/49029 [==============================] - 4s 85us/step - loss: 0.2440 - acc: 0.9124 - val_loss: 0.2401 - val_acc: 0.9125\n",
      "Epoch 156/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.2429 - acc: 0.9130 - val_loss: 0.2456 - val_acc: 0.9083\n",
      "Epoch 157/300\n",
      "49029/49029 [==============================] - 5s 99us/step - loss: 0.2435 - acc: 0.9120 - val_loss: 0.2377 - val_acc: 0.9128\n",
      "Epoch 158/300\n",
      "49029/49029 [==============================] - 5s 101us/step - loss: 0.2434 - acc: 0.9123 - val_loss: 0.2403 - val_acc: 0.9135\n",
      "Epoch 159/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2436 - acc: 0.9127 - val_loss: 0.2388 - val_acc: 0.9129\n",
      "Epoch 160/300\n",
      "49029/49029 [==============================] - 5s 95us/step - loss: 0.2434 - acc: 0.9119 - val_loss: 0.2461 - val_acc: 0.9082\n",
      "Epoch 161/300\n",
      "49029/49029 [==============================] - 4s 88us/step - loss: 0.2434 - acc: 0.9124 - val_loss: 0.2395 - val_acc: 0.9147\n",
      "Epoch 162/300\n",
      "49029/49029 [==============================] - 5s 108us/step - loss: 0.2440 - acc: 0.9124 - val_loss: 0.2431 - val_acc: 0.9127\n",
      "Epoch 163/300\n",
      "49029/49029 [==============================] - 6s 126us/step - loss: 0.2433 - acc: 0.9124 - val_loss: 0.2440 - val_acc: 0.9137\n",
      "Epoch 164/300\n",
      "49029/49029 [==============================] - 7s 134us/step - loss: 0.2429 - acc: 0.9127 - val_loss: 0.2525 - val_acc: 0.9094\n",
      "Epoch 165/300\n",
      "49029/49029 [==============================] - 6s 118us/step - loss: 0.2431 - acc: 0.9125 - val_loss: 0.2400 - val_acc: 0.9141\n",
      "Epoch 166/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2426 - acc: 0.9130 - val_loss: 0.2452 - val_acc: 0.9094\n",
      "Epoch 167/300\n",
      "49029/49029 [==============================] - 5s 105us/step - loss: 0.2430 - acc: 0.9129 - val_loss: 0.2477 - val_acc: 0.9144\n",
      "Epoch 168/300\n",
      "49029/49029 [==============================] - 3s 67us/step - loss: 0.2432 - acc: 0.9128 - val_loss: 0.2403 - val_acc: 0.9154\n",
      "Epoch 169/300\n",
      "49029/49029 [==============================] - 3s 65us/step - loss: 0.2435 - acc: 0.9126 - val_loss: 0.2469 - val_acc: 0.9118\n",
      "Epoch 170/300\n",
      "49029/49029 [==============================] - 3s 63us/step - loss: 0.2437 - acc: 0.9121 - val_loss: 0.2394 - val_acc: 0.9144\n",
      "Epoch 171/300\n",
      "49029/49029 [==============================] - 3s 62us/step - loss: 0.2424 - acc: 0.9125 - val_loss: 0.2392 - val_acc: 0.9127\n",
      "Epoch 172/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2419 - acc: 0.9129 - val_loss: 0.2453 - val_acc: 0.9133\n",
      "Epoch 173/300\n",
      "49029/49029 [==============================] - 3s 65us/step - loss: 0.2431 - acc: 0.9127 - val_loss: 0.2378 - val_acc: 0.9147\n",
      "Epoch 174/300\n",
      "49029/49029 [==============================] - 3s 63us/step - loss: 0.2418 - acc: 0.9131 - val_loss: 0.2361 - val_acc: 0.9144\n",
      "Epoch 175/300\n",
      "49029/49029 [==============================] - 3s 62us/step - loss: 0.2415 - acc: 0.9130 - val_loss: 0.2402 - val_acc: 0.9138\n",
      "Epoch 176/300\n",
      "49029/49029 [==============================] - 3s 65us/step - loss: 0.2417 - acc: 0.9133 - val_loss: 0.2382 - val_acc: 0.9137\n",
      "Epoch 177/300\n",
      "49029/49029 [==============================] - 3s 61us/step - loss: 0.2423 - acc: 0.9131 - val_loss: 0.2412 - val_acc: 0.9103\n",
      "Epoch 178/300\n",
      "49029/49029 [==============================] - 3s 61us/step - loss: 0.2419 - acc: 0.9125 - val_loss: 0.2407 - val_acc: 0.9124\n",
      "Epoch 179/300\n",
      "49029/49029 [==============================] - 3s 66us/step - loss: 0.2412 - acc: 0.9134 - val_loss: 0.2404 - val_acc: 0.9099\n",
      "Epoch 180/300\n",
      "49029/49029 [==============================] - 3s 64us/step - loss: 0.2412 - acc: 0.9131 - val_loss: 0.2439 - val_acc: 0.9140\n",
      "Epoch 181/300\n",
      "49029/49029 [==============================] - 3s 66us/step - loss: 0.2419 - acc: 0.9131 - val_loss: 0.2449 - val_acc: 0.9147\n",
      "Epoch 182/300\n",
      "49029/49029 [==============================] - 3s 65us/step - loss: 0.2413 - acc: 0.9131 - val_loss: 0.2431 - val_acc: 0.9149\n",
      "Epoch 183/300\n",
      "49029/49029 [==============================] - 3s 62us/step - loss: 0.2416 - acc: 0.9124 - val_loss: 0.2370 - val_acc: 0.9150\n",
      "Epoch 184/300\n",
      "49029/49029 [==============================] - 3s 65us/step - loss: 0.2414 - acc: 0.9131 - val_loss: 0.2357 - val_acc: 0.9124\n",
      "Epoch 185/300\n",
      "49029/49029 [==============================] - 4s 86us/step - loss: 0.2410 - acc: 0.9134 - val_loss: 0.2369 - val_acc: 0.9148\n",
      "Epoch 186/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.2418 - acc: 0.9127 - val_loss: 0.2377 - val_acc: 0.9140\n",
      "Epoch 187/300\n",
      "49029/49029 [==============================] - 5s 97us/step - loss: 0.2414 - acc: 0.9131 - val_loss: 0.2373 - val_acc: 0.9128\n",
      "Epoch 188/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.2409 - acc: 0.9126 - val_loss: 0.2362 - val_acc: 0.9130\n",
      "Epoch 189/300\n",
      "49029/49029 [==============================] - 4s 88us/step - loss: 0.2410 - acc: 0.9131 - val_loss: 0.2373 - val_acc: 0.9143\n",
      "Epoch 190/300\n",
      "49029/49029 [==============================] - 4s 85us/step - loss: 0.2410 - acc: 0.9134 - val_loss: 0.2358 - val_acc: 0.9137\n",
      "Epoch 191/300\n",
      "49029/49029 [==============================] - 4s 86us/step - loss: 0.2409 - acc: 0.9129 - val_loss: 0.2451 - val_acc: 0.9138\n",
      "Epoch 192/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2409 - acc: 0.9132 - val_loss: 0.2400 - val_acc: 0.9140\n",
      "Epoch 193/300\n",
      "49029/49029 [==============================] - 5s 99us/step - loss: 0.2400 - acc: 0.9129 - val_loss: 0.2353 - val_acc: 0.9152\n",
      "Epoch 194/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2394 - acc: 0.9136 - val_loss: 0.2403 - val_acc: 0.9135\n",
      "Epoch 195/300\n",
      "49029/49029 [==============================] - 4s 86us/step - loss: 0.2401 - acc: 0.9134 - val_loss: 0.2366 - val_acc: 0.9135\n",
      "Epoch 196/300\n",
      "49029/49029 [==============================] - 4s 81us/step - loss: 0.2403 - acc: 0.9130 - val_loss: 0.2391 - val_acc: 0.9153\n",
      "Epoch 197/300\n",
      "49029/49029 [==============================] - 4s 91us/step - loss: 0.2395 - acc: 0.9134 - val_loss: 0.2363 - val_acc: 0.9149\n",
      "Epoch 198/300\n",
      "49029/49029 [==============================] - 5s 102us/step - loss: 0.2399 - acc: 0.9132 - val_loss: 0.2372 - val_acc: 0.9140\n",
      "Epoch 199/300\n",
      "49029/49029 [==============================] - 4s 90us/step - loss: 0.2398 - acc: 0.9135 - val_loss: 0.2384 - val_acc: 0.9136\n",
      "Epoch 200/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.2404 - acc: 0.9137 - val_loss: 0.2369 - val_acc: 0.9137\n",
      "Epoch 201/300\n",
      "49029/49029 [==============================] - 6s 114us/step - loss: 0.2400 - acc: 0.9134 - val_loss: 0.2379 - val_acc: 0.9144\n",
      "Epoch 202/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2399 - acc: 0.9133 - val_loss: 0.2358 - val_acc: 0.9137\n",
      "Epoch 203/300\n",
      "49029/49029 [==============================] - 6s 120us/step - loss: 0.2403 - acc: 0.9129 - val_loss: 0.2362 - val_acc: 0.9160\n",
      "Epoch 204/300\n",
      "49029/49029 [==============================] - 15s 315us/step - loss: 0.2397 - acc: 0.9126 - val_loss: 0.2382 - val_acc: 0.9142\n",
      "Epoch 205/300\n",
      "49029/49029 [==============================] - 10s 205us/step - loss: 0.2392 - acc: 0.9129 - val_loss: 0.2350 - val_acc: 0.9148\n",
      "Epoch 206/300\n",
      "49029/49029 [==============================] - 5s 104us/step - loss: 0.2385 - acc: 0.9131 - val_loss: 0.2455 - val_acc: 0.9100\n",
      "Epoch 207/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2390 - acc: 0.9130 - val_loss: 0.2393 - val_acc: 0.9115\n",
      "Epoch 208/300\n",
      "49029/49029 [==============================] - 5s 98us/step - loss: 0.2398 - acc: 0.9129 - val_loss: 0.2451 - val_acc: 0.9118\n",
      "Epoch 209/300\n",
      "49029/49029 [==============================] - 5s 95us/step - loss: 0.2391 - acc: 0.9136 - val_loss: 0.2445 - val_acc: 0.9125\n",
      "Epoch 210/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2393 - acc: 0.9129 - val_loss: 0.2356 - val_acc: 0.9148\n",
      "Epoch 211/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2383 - acc: 0.9135 - val_loss: 0.2347 - val_acc: 0.9140\n",
      "Epoch 212/300\n",
      "49029/49029 [==============================] - 5s 100us/step - loss: 0.2383 - acc: 0.9137 - val_loss: 0.2368 - val_acc: 0.9150\n",
      "Epoch 213/300\n",
      "49029/49029 [==============================] - 5s 103us/step - loss: 0.2382 - acc: 0.9134 - val_loss: 0.2398 - val_acc: 0.9150\n",
      "Epoch 214/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.2390 - acc: 0.9134 - val_loss: 0.2480 - val_acc: 0.9082\n",
      "Epoch 215/300\n",
      "49029/49029 [==============================] - 4s 85us/step - loss: 0.2378 - acc: 0.9133 - val_loss: 0.2394 - val_acc: 0.9150\n",
      "Epoch 216/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2381 - acc: 0.9134 - val_loss: 0.2357 - val_acc: 0.9143\n",
      "Epoch 217/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2391 - acc: 0.9136 - val_loss: 0.2386 - val_acc: 0.9161\n",
      "Epoch 218/300\n",
      "49029/49029 [==============================] - 3s 66us/step - loss: 0.2385 - acc: 0.9129 - val_loss: 0.2402 - val_acc: 0.9115\n",
      "Epoch 219/300\n",
      "49029/49029 [==============================] - 5s 102us/step - loss: 0.2381 - acc: 0.9133 - val_loss: 0.2404 - val_acc: 0.9140\n",
      "Epoch 220/300\n",
      "49029/49029 [==============================] - 5s 106us/step - loss: 0.2392 - acc: 0.9135 - val_loss: 0.2343 - val_acc: 0.9152\n",
      "Epoch 221/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2378 - acc: 0.9132 - val_loss: 0.2349 - val_acc: 0.9151\n",
      "Epoch 222/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2371 - acc: 0.9137 - val_loss: 0.2359 - val_acc: 0.9094\n",
      "Epoch 223/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2386 - acc: 0.9129 - val_loss: 0.2372 - val_acc: 0.9125\n",
      "Epoch 224/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2374 - acc: 0.9141 - val_loss: 0.2383 - val_acc: 0.9131\n",
      "Epoch 225/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2378 - acc: 0.9136 - val_loss: 0.2398 - val_acc: 0.9135\n",
      "Epoch 226/300\n",
      "49029/49029 [==============================] - 3s 64us/step - loss: 0.2370 - acc: 0.9138 - val_loss: 0.2370 - val_acc: 0.9149\n",
      "Epoch 227/300\n",
      "49029/49029 [==============================] - 3s 62us/step - loss: 0.2377 - acc: 0.9133 - val_loss: 0.2349 - val_acc: 0.9149\n",
      "Epoch 228/300\n",
      "49029/49029 [==============================] - 3s 68us/step - loss: 0.2378 - acc: 0.9138 - val_loss: 0.2366 - val_acc: 0.9152\n",
      "Epoch 229/300\n",
      "49029/49029 [==============================] - 3s 63us/step - loss: 0.2375 - acc: 0.9132 - val_loss: 0.2439 - val_acc: 0.9127\n",
      "Epoch 230/300\n",
      "49029/49029 [==============================] - 3s 63us/step - loss: 0.2379 - acc: 0.9130 - val_loss: 0.2346 - val_acc: 0.9150\n",
      "Epoch 231/300\n",
      "49029/49029 [==============================] - 3s 62us/step - loss: 0.2366 - acc: 0.9137 - val_loss: 0.2367 - val_acc: 0.9136\n",
      "Epoch 232/300\n",
      "49029/49029 [==============================] - 3s 63us/step - loss: 0.2379 - acc: 0.9136 - val_loss: 0.2354 - val_acc: 0.9154\n",
      "Epoch 233/300\n",
      "49029/49029 [==============================] - 3s 64us/step - loss: 0.2369 - acc: 0.9137 - val_loss: 0.2404 - val_acc: 0.9135\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49029/49029 [==============================] - 3s 64us/step - loss: 0.2372 - acc: 0.9133 - val_loss: 0.2360 - val_acc: 0.9174\n",
      "Epoch 235/300\n",
      "49029/49029 [==============================] - 3s 65us/step - loss: 0.2367 - acc: 0.9132 - val_loss: 0.2365 - val_acc: 0.9147\n",
      "Epoch 236/300\n",
      "49029/49029 [==============================] - 3s 63us/step - loss: 0.2377 - acc: 0.9134 - val_loss: 0.2338 - val_acc: 0.9147\n",
      "Epoch 237/300\n",
      "49029/49029 [==============================] - 3s 63us/step - loss: 0.2369 - acc: 0.9141 - val_loss: 0.2351 - val_acc: 0.9146\n",
      "Epoch 238/300\n",
      "49029/49029 [==============================] - 3s 64us/step - loss: 0.2367 - acc: 0.9138 - val_loss: 0.2413 - val_acc: 0.9110\n",
      "Epoch 239/300\n",
      "49029/49029 [==============================] - 3s 64us/step - loss: 0.2369 - acc: 0.9128 - val_loss: 0.2327 - val_acc: 0.9155\n",
      "Epoch 240/300\n",
      "49029/49029 [==============================] - 3s 64us/step - loss: 0.2361 - acc: 0.9138 - val_loss: 0.2359 - val_acc: 0.9152\n",
      "Epoch 241/300\n",
      "49029/49029 [==============================] - 4s 88us/step - loss: 0.2367 - acc: 0.9138 - val_loss: 0.2385 - val_acc: 0.9157\n",
      "Epoch 242/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2359 - acc: 0.9145 - val_loss: 0.2340 - val_acc: 0.9133\n",
      "Epoch 243/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.2367 - acc: 0.9144 - val_loss: 0.2343 - val_acc: 0.9152\n",
      "Epoch 244/300\n",
      "49029/49029 [==============================] - 4s 71us/step - loss: 0.2368 - acc: 0.9131 - val_loss: 0.2328 - val_acc: 0.9138\n",
      "Epoch 245/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2362 - acc: 0.9132 - val_loss: 0.2334 - val_acc: 0.9137\n",
      "Epoch 246/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2366 - acc: 0.9135 - val_loss: 0.2359 - val_acc: 0.9163\n",
      "Epoch 247/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.2366 - acc: 0.9132 - val_loss: 0.2540 - val_acc: 0.9109\n",
      "Epoch 248/300\n",
      "49029/49029 [==============================] - 6s 116us/step - loss: 0.2369 - acc: 0.9132 - val_loss: 0.2392 - val_acc: 0.9156\n",
      "Epoch 249/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.2364 - acc: 0.9137 - val_loss: 0.2338 - val_acc: 0.9163\n",
      "Epoch 250/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2364 - acc: 0.9137 - val_loss: 0.2320 - val_acc: 0.9168\n",
      "Epoch 251/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2355 - acc: 0.9137 - val_loss: 0.2419 - val_acc: 0.9123\n",
      "Epoch 252/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2355 - acc: 0.9143 - val_loss: 0.2364 - val_acc: 0.9145\n",
      "Epoch 253/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2363 - acc: 0.9138 - val_loss: 0.2344 - val_acc: 0.9150\n",
      "Epoch 254/300\n",
      "49029/49029 [==============================] - 4s 90us/step - loss: 0.2362 - acc: 0.9139 - val_loss: 0.2336 - val_acc: 0.9165\n",
      "Epoch 255/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.2356 - acc: 0.9142 - val_loss: 0.2405 - val_acc: 0.9143\n",
      "Epoch 256/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.2359 - acc: 0.9145 - val_loss: 0.2352 - val_acc: 0.9171\n",
      "Epoch 257/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2357 - acc: 0.9144 - val_loss: 0.2335 - val_acc: 0.9150\n",
      "Epoch 258/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2354 - acc: 0.9146 - val_loss: 0.2366 - val_acc: 0.9174\n",
      "Epoch 259/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2361 - acc: 0.9139 - val_loss: 0.2378 - val_acc: 0.9155\n",
      "Epoch 260/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2351 - acc: 0.9149 - val_loss: 0.2353 - val_acc: 0.9175\n",
      "Epoch 261/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2363 - acc: 0.9138 - val_loss: 0.2357 - val_acc: 0.9158\n",
      "Epoch 262/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2358 - acc: 0.9137 - val_loss: 0.2341 - val_acc: 0.9158\n",
      "Epoch 263/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2358 - acc: 0.9150 - val_loss: 0.2355 - val_acc: 0.9157\n",
      "Epoch 264/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2348 - acc: 0.9145 - val_loss: 0.2482 - val_acc: 0.9073\n",
      "Epoch 265/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2351 - acc: 0.9140 - val_loss: 0.2315 - val_acc: 0.9160\n",
      "Epoch 266/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2348 - acc: 0.9143 - val_loss: 0.2340 - val_acc: 0.9134\n",
      "Epoch 267/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2347 - acc: 0.9143 - val_loss: 0.2378 - val_acc: 0.9116\n",
      "Epoch 268/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2355 - acc: 0.9141 - val_loss: 0.2347 - val_acc: 0.9155\n",
      "Epoch 269/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2357 - acc: 0.9143 - val_loss: 0.2454 - val_acc: 0.9132\n",
      "Epoch 270/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2348 - acc: 0.9138 - val_loss: 0.2397 - val_acc: 0.9147\n",
      "Epoch 271/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2347 - acc: 0.9141 - val_loss: 0.2345 - val_acc: 0.9150\n",
      "Epoch 272/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2352 - acc: 0.9142 - val_loss: 0.2445 - val_acc: 0.9071\n",
      "Epoch 273/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.2351 - acc: 0.9140 - val_loss: 0.2391 - val_acc: 0.9131\n",
      "Epoch 274/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.2345 - acc: 0.9144 - val_loss: 0.2364 - val_acc: 0.9127\n",
      "Epoch 275/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2348 - acc: 0.9143 - val_loss: 0.2327 - val_acc: 0.9192\n",
      "Epoch 276/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.2354 - acc: 0.9137 - val_loss: 0.2410 - val_acc: 0.9115\n",
      "Epoch 277/300\n",
      "49029/49029 [==============================] - 6s 116us/step - loss: 0.2348 - acc: 0.9139 - val_loss: 0.2371 - val_acc: 0.9162\n",
      "Epoch 278/300\n",
      "49029/49029 [==============================] - 5s 94us/step - loss: 0.2348 - acc: 0.9147 - val_loss: 0.2505 - val_acc: 0.9130\n",
      "Epoch 279/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.2349 - acc: 0.9141 - val_loss: 0.2372 - val_acc: 0.9150\n",
      "Epoch 280/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.2348 - acc: 0.9139 - val_loss: 0.2390 - val_acc: 0.9120\n",
      "Epoch 281/300\n",
      "49029/49029 [==============================] - 5s 92us/step - loss: 0.2343 - acc: 0.9137 - val_loss: 0.2396 - val_acc: 0.9135\n",
      "Epoch 282/300\n",
      "49029/49029 [==============================] - 4s 86us/step - loss: 0.2338 - acc: 0.9143 - val_loss: 0.2346 - val_acc: 0.9174\n",
      "Epoch 283/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2345 - acc: 0.9150 - val_loss: 0.2384 - val_acc: 0.9181\n",
      "Epoch 284/300\n",
      "49029/49029 [==============================] - 4s 91us/step - loss: 0.2344 - acc: 0.9138 - val_loss: 0.2315 - val_acc: 0.9153\n",
      "Epoch 285/300\n",
      "49029/49029 [==============================] - 4s 91us/step - loss: 0.2341 - acc: 0.9151 - val_loss: 0.2341 - val_acc: 0.9147\n",
      "Epoch 286/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.2341 - acc: 0.9147 - val_loss: 0.2337 - val_acc: 0.9152\n",
      "Epoch 287/300\n",
      "49029/49029 [==============================] - 5s 93us/step - loss: 0.2346 - acc: 0.9138 - val_loss: 0.2373 - val_acc: 0.9136\n",
      "Epoch 288/300\n",
      "49029/49029 [==============================] - 5s 100us/step - loss: 0.2350 - acc: 0.9149 - val_loss: 0.2333 - val_acc: 0.9159\n",
      "Epoch 289/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2340 - acc: 0.9148 - val_loss: 0.2318 - val_acc: 0.9163\n",
      "Epoch 290/300\n",
      "49029/49029 [==============================] - 4s 90us/step - loss: 0.2352 - acc: 0.9141 - val_loss: 0.2334 - val_acc: 0.9171\n",
      "Epoch 291/300\n",
      "49029/49029 [==============================] - 4s 81us/step - loss: 0.2341 - acc: 0.9143 - val_loss: 0.2305 - val_acc: 0.9166\n",
      "Epoch 292/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2338 - acc: 0.9140 - val_loss: 0.2407 - val_acc: 0.9138\n",
      "Epoch 293/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.2341 - acc: 0.9143 - val_loss: 0.2370 - val_acc: 0.9136\n",
      "Epoch 294/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.2342 - acc: 0.9145 - val_loss: 0.2366 - val_acc: 0.9156\n",
      "Epoch 295/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2342 - acc: 0.9153 - val_loss: 0.2392 - val_acc: 0.9127\n",
      "Epoch 296/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2343 - acc: 0.9141 - val_loss: 0.2448 - val_acc: 0.9101\n",
      "Epoch 297/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2337 - acc: 0.9147 - val_loss: 0.2357 - val_acc: 0.9163\n",
      "Epoch 298/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2334 - acc: 0.9149 - val_loss: 0.2351 - val_acc: 0.9140\n",
      "Epoch 299/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.2335 - acc: 0.9144 - val_loss: 0.2325 - val_acc: 0.9141\n",
      "Epoch 300/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2340 - acc: 0.9145 - val_loss: 0.2370 - val_acc: 0.9108\n",
      "73178/73178 [==============================] - 2s 28us/step\n",
      "18295/18295 [==============================] - 1s 33us/step\n",
      "Training set accuracy: 0.9120637350039521\n",
      "Training set loss: 0.23598015509280473\n",
      "Test set accuracy: 0.9120524733794391\n",
      "Test set loss: 0.23658748286837078\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(60, activation='sigmoid', input_dim = 4))\n",
    "model.add(Dense(30,activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# can try rmsprop\n",
    "model.compile(optimizer = 'adam',     \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy']) \n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_split = 0.33, \n",
    "                    epochs = 300, \n",
    "                    batch_size = 32)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "print('Training set accuracy:', train_acc)\n",
    "print('Training set loss:', train_loss)\n",
    "\n",
    "print('Test set accuracy:', test_acc)\n",
    "print('Test set loss:', test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The history of our accuracy during training.\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The history of our cross-entropy loss during training.\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on Test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "true_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "#also remember to pickle model for kyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Decision Tree\n",
    "\n",
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc = dtc.fit(X_train,y_train)\n",
    "print(\"Decision Tree Accuracy:\", dtc_score)\n",
    "\n",
    "true_test_X = true_test[['app', 'device', 'os', 'channel']]\n",
    "\n",
    "true_test_predictions = dtc.predict(true_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"ID\": true_test[\"click_id\"],\n",
    "        \"is_attributed\": true_test_predictions\n",
    "    })\n",
    "# submission.to_csv('../output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
