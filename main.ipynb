{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adtracking Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Kyle O'Brien, Catherine Lee, Amit Saxena "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is in \"data/\" and includes the training and testing csv files.\n",
    "Evaluation metrics will include looking at R^2 and a confusion matrix. Other things to try, CNN (Resnet, inceptionv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np         # linear algebra\n",
    "import sklearn as sk       # machine learning\n",
    "import pandas as pd        # reading in data files, data cleaning\n",
    "import matplotlib.pyplot as plt   # for plotting\n",
    "import seaborn as sns      # visualization tool\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_sample: 100,000 randomly-selected rows of training data (because the full training data takes too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83252</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:42:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106590</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:43:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147164</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>134</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39782</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>205</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121646</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel           click_time attributed_time  \\\n",
       "0   83252    3       1  16      379  2017-11-06 15:42:19             NaN   \n",
       "1  106590    3       1  25      379  2017-11-06 15:43:23             NaN   \n",
       "2  147164   14       1  28      134  2017-11-06 16:00:00             NaN   \n",
       "3   39782    2       1  10      205  2017-11-06 16:00:00             NaN   \n",
       "4  121646   23       1  13      153  2017-11-06 16:00:00             NaN   \n",
       "\n",
       "   is_attributed  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data in \n",
    "\n",
    "data = pd.read_csv(\"data/equalized_train.csv\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91473 entries, 0 to 91472\n",
      "Data columns (total 8 columns):\n",
      "ip                 91473 non-null int64\n",
      "app                91473 non-null int64\n",
      "device             91473 non-null int64\n",
      "os                 91473 non-null int64\n",
      "channel            91473 non-null int64\n",
      "click_time         91473 non-null object\n",
      "attributed_time    41473 non-null object\n",
      "is_attributed      91473 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping attributed_time because many null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsed `click_time` feature to seconds since epoch\n",
    "# Stored new column in `click_time_epoch`\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def epoch_distance(iso):\n",
    "    dt = datetime.strptime(iso,\"%Y-%m-%d %H:%M:%S\")\n",
    "    dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.timestamp()\n",
    "\n",
    "#convert click_time to seconds since epoch\n",
    "data['click_time_epoch'] =  data['click_time'].map(epoch_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_seconds(timestamp):\n",
    "    dt = datetime.fromtimestamp(timestamp,timezone.utc)\n",
    "    return dt.hour*3600 + dt.minute*60 + dt.second\n",
    "\n",
    "def day_minutes(timestamp):\n",
    "    return round(day_seconds(timestamp) / 60)\n",
    "    \n",
    "\n",
    "# parsed `click_time` feature to seconds of the current day\n",
    "# stored new column in `click_time_secs`\n",
    "data['click_time_secs'] =  data['click_time_epoch'].map(day_seconds)\n",
    "\n",
    "# parsed `click_time` feature to minutes of the current day\n",
    "# stored new column in `click_time_mins`\n",
    "data['click_time_mins'] =  data['click_time_epoch'].map(day_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91473 entries, 0 to 91472\n",
      "Data columns (total 11 columns):\n",
      "ip                  91473 non-null int64\n",
      "app                 91473 non-null int64\n",
      "device              91473 non-null int64\n",
      "os                  91473 non-null int64\n",
      "channel             91473 non-null int64\n",
      "click_time          91473 non-null object\n",
      "attributed_time     41473 non-null object\n",
      "is_attributed       91473 non-null int64\n",
      "click_time_epoch    91473 non-null float64\n",
      "click_time_secs     91473 non-null int64\n",
      "click_time_mins     91473 non-null int64\n",
      "dtypes: float64(1), int64(8), object(2)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>click_time_epoch</th>\n",
       "      <th>click_time_secs</th>\n",
       "      <th>click_time_mins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83252</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:42:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.509983e+09</td>\n",
       "      <td>56539</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106590</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:43:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.509983e+09</td>\n",
       "      <td>56603</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147164</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>134</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.509984e+09</td>\n",
       "      <td>57600</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39782</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>205</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.509984e+09</td>\n",
       "      <td>57600</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121646</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.509984e+09</td>\n",
       "      <td>57600</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel           click_time attributed_time  \\\n",
       "0   83252    3       1  16      379  2017-11-06 15:42:19             NaN   \n",
       "1  106590    3       1  25      379  2017-11-06 15:43:23             NaN   \n",
       "2  147164   14       1  28      134  2017-11-06 16:00:00             NaN   \n",
       "3   39782    2       1  10      205  2017-11-06 16:00:00             NaN   \n",
       "4  121646   23       1  13      153  2017-11-06 16:00:00             NaN   \n",
       "\n",
       "   is_attributed  click_time_epoch  click_time_secs  click_time_mins  \n",
       "0              0      1.509983e+09            56539              942  \n",
       "1              0      1.509983e+09            56603              943  \n",
       "2              0      1.509984e+09            57600              960  \n",
       "3              0      1.509984e+09            57600              960  \n",
       "4              0      1.509984e+09            57600              960  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.drop(columns=['attributed_time'])\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    91473.000000\n",
       "mean         0.453391\n",
       "std          0.497826\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: is_attributed, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attributed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_attributed\n",
      "0    50000\n",
      "1    41473\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#plotx = data['attributed_time']\n",
    "#ploty = data['is_attributed']\n",
    "\n",
    "#plt.scatter(plotx,ploty)\n",
    "import pandas\n",
    "class_counts = data.groupby('is_attributed').size()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that we have a major imbalance in our data. This is incentivising our model to guess 0 for overthing and it would still receive a 99% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['app', 'device', 'os', 'channel']]\n",
    "y = data['is_attributed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22243</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58933</th>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50574</th>\n",
       "      <td>18</td>\n",
       "      <td>3032</td>\n",
       "      <td>607</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63812</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52847</th>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       app  device   os  channel\n",
       "22243    2       1   20      477\n",
       "58933  105       1   17      451\n",
       "50574   18    3032  607      107\n",
       "63812   11       1   13      325\n",
       "52847  116       1   41      101"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22243    0\n",
       "58933    1\n",
       "50574    0\n",
       "63812    1\n",
       "52847    1\n",
       "Name: is_attributed, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems a little strange that we would get such high accuracy. This might have to do with the fact that we are using R^2 to measure the goodness of our model. There might be an imbalance in the test set of which clicks are 0 or 1, so we should try precision and recall. (Longterm with full training data, try to keep an equal number of classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy 0.6349822355834928\n"
     ]
    }
   ],
   "source": [
    "logreg_score = logreg.score(X_test, y_test)\n",
    "print(\"Logistic Regression Accuracy\", logreg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy: 0.933588412134463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier()\n",
    "RandomForest.fit(X_train,y_train)\n",
    "rf_score = RandomForest.score(X_test,y_test)\n",
    "print(\"Random Forest accuracy:\", rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      9971\n",
      "           1       0.96      0.89      0.92      8324\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     18295\n",
      "   macro avg       0.94      0.93      0.93     18295\n",
      "weighted avg       0.94      0.93      0.93     18295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall Score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = RandomForest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lSVC = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: 0.4994260726974583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lSVC.fit(X_train,y_train)\n",
    "svm_score = lSVC.score(X_test,y_test)\n",
    "print(\"SVM accuracy:\", svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9352282044274391\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "\n",
    "dtc = dtc.fit(X_train,y_train)\n",
    "dtc_score =  dtc.score(X_test,y_test)\n",
    "print(\"Decision Tree Accuracy:\", dtc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      9971\n",
      "           1       0.96      0.89      0.93      8324\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     18295\n",
      "   macro avg       0.94      0.93      0.93     18295\n",
      "weighted avg       0.94      0.94      0.93     18295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall Score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9689  282]\n",
      " [ 903 7421]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "#In Progress\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "#plt.title(\"Confusion Matrix\")\n",
    "\n",
    "#plt.colorbar()\n",
    "#tick_marks = np.arange(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "knn_score = knn.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, y_train)\n",
    "Y_pred = gaussian.predict(X_test)\n",
    "gaussian_score = gaussian.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train)\n",
    "Y_pred = sgd.predict(X_test)\n",
    "sgd_score = sgd.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "Y_pred = perceptron.predict(X_test)\n",
    "perc_score = perceptron.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "XGB = xgb.XGBClassifier()\n",
    "XGB.fit(X_train, y_train)\n",
    "Y_pred = XGB.predict(X_test)\n",
    "XGB_score = XGB.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbt = GradientBoostingClassifier()\n",
    "gbt = gbt.fit(X_train, y_train)\n",
    "gbt_acc = gbt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier/ Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#dictionary of our models\n",
    "model1 = xgb.XGBClassifier()\n",
    "model2 = tree.DecisionTreeClassifier()\n",
    "model3 = RandomForestClassifier() \n",
    "estimators=[(‘xgboost’, model1), (‘decisiontree’, model2), (‘randomforest’, model3)]\n",
    "\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting=’hard’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=100).fit(X_train,y_train)\n",
    "adaboost_acc = adaboost.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag = BaggingClassifier()\n",
    "bag = bag.fit(X_train,y_train)\n",
    "bag_acc = bag.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "xtrees = ExtraTreesClassifier.fit(X_train,y_train)\n",
    "xtrees_acc = xtrees.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which model do we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.935228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.933698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.933588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Boosted Trees</td>\n",
       "      <td>0.924460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K Nearest Neighbors</td>\n",
       "      <td>0.921235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.921181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>0.914949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.688385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.634982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.615742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.612845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.499426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.499426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model     Score\n",
       "4                 Decision Tree  0.935228\n",
       "12           Bagging Classifier  0.933698\n",
       "2                 Random Forest  0.933588\n",
       "10       Gradient Boosted Trees  0.924460\n",
       "5           K Nearest Neighbors  0.921235\n",
       "9                       XGBoost  0.921181\n",
       "11                     Adaboost  0.914949\n",
       "6          Gaussian Naive Bayes  0.688385\n",
       "1           Logistic Regression  0.634982\n",
       "7   Stochastic Gradient Descent  0.615742\n",
       "8                    Perceptron  0.612845\n",
       "0       Support Vector Machines  0.499426\n",
       "3                    Linear SVC  0.499426"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'Logistic Regression', \n",
    "              'Random Forest', 'Linear SVC', \n",
    "              'Decision Tree', 'K Nearest Neighbors', \n",
    "              'Gaussian Naive Bayes', 'Stochastic Gradient Descent', \n",
    "              'Perceptron', 'XGBoost', 'Gradient Boosted Trees', 'Adaboost', 'Bagging Classifier'],\n",
    "    'Score': [svm_score, logreg_score, rf_score, svm_score, dtc_score, \n",
    "              knn_score, gaussian_score, sgd_score, perc_score, \n",
    "              XGB_score, gbt_acc, adaboost_acc, bag_acc]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49029 samples, validate on 24149 samples\n",
      "Epoch 1/300\n",
      "49029/49029 [==============================] - 5s 110us/step - loss: 0.5590 - acc: 0.6965 - val_loss: 0.5139 - val_acc: 0.7342\n",
      "Epoch 2/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.5006 - acc: 0.7707 - val_loss: 0.4792 - val_acc: 0.8164\n",
      "Epoch 3/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.4607 - acc: 0.8067 - val_loss: 0.4379 - val_acc: 0.8120\n",
      "Epoch 4/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.4272 - acc: 0.8247 - val_loss: 0.4006 - val_acc: 0.8413\n",
      "Epoch 5/300\n",
      "49029/49029 [==============================] - 4s 80us/step - loss: 0.3878 - acc: 0.8536 - val_loss: 0.3764 - val_acc: 0.8513\n",
      "Epoch 6/300\n",
      "49029/49029 [==============================] - 4s 88us/step - loss: 0.3601 - acc: 0.8679 - val_loss: 0.3415 - val_acc: 0.8700\n",
      "Epoch 7/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.3423 - acc: 0.8710 - val_loss: 0.3295 - val_acc: 0.8763\n",
      "Epoch 8/300\n",
      "49029/49029 [==============================] - 5s 96us/step - loss: 0.3338 - acc: 0.8741 - val_loss: 0.3221 - val_acc: 0.8815\n",
      "Epoch 9/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.3258 - acc: 0.8787 - val_loss: 0.3125 - val_acc: 0.8822\n",
      "Epoch 10/300\n",
      "49029/49029 [==============================] - 4s 91us/step - loss: 0.3185 - acc: 0.8815 - val_loss: 0.3074 - val_acc: 0.8797\n",
      "Epoch 11/300\n",
      "49029/49029 [==============================] - 5s 97us/step - loss: 0.3137 - acc: 0.8839 - val_loss: 0.3044 - val_acc: 0.8917\n",
      "Epoch 12/300\n",
      "49029/49029 [==============================] - 5s 102us/step - loss: 0.3093 - acc: 0.8864 - val_loss: 0.3016 - val_acc: 0.8844\n",
      "Epoch 13/300\n",
      "49029/49029 [==============================] - 5s 98us/step - loss: 0.3063 - acc: 0.8876 - val_loss: 0.2926 - val_acc: 0.8961\n",
      "Epoch 14/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.3023 - acc: 0.8911 - val_loss: 0.2886 - val_acc: 0.8992\n",
      "Epoch 15/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2975 - acc: 0.8941 - val_loss: 0.2821 - val_acc: 0.8970\n",
      "Epoch 16/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2932 - acc: 0.8959 - val_loss: 0.2836 - val_acc: 0.9006\n",
      "Epoch 17/300\n",
      "49029/49029 [==============================] - 5s 109us/step - loss: 0.2893 - acc: 0.8987 - val_loss: 0.2746 - val_acc: 0.9039\n",
      "Epoch 18/300\n",
      "49029/49029 [==============================] - 4s 84us/step - loss: 0.2878 - acc: 0.9003 - val_loss: 0.3049 - val_acc: 0.8911\n",
      "Epoch 19/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2858 - acc: 0.9008 - val_loss: 0.2762 - val_acc: 0.9055\n",
      "Epoch 20/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2843 - acc: 0.9007 - val_loss: 0.2711 - val_acc: 0.9051\n",
      "Epoch 21/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2836 - acc: 0.9005 - val_loss: 0.2704 - val_acc: 0.9064\n",
      "Epoch 22/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2809 - acc: 0.9025 - val_loss: 0.2719 - val_acc: 0.9055\n",
      "Epoch 23/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2807 - acc: 0.9028 - val_loss: 0.2790 - val_acc: 0.9012\n",
      "Epoch 24/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2825 - acc: 0.9013 - val_loss: 0.2672 - val_acc: 0.9089\n",
      "Epoch 25/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2797 - acc: 0.9037 - val_loss: 0.2680 - val_acc: 0.9082\n",
      "Epoch 26/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2792 - acc: 0.9033 - val_loss: 0.2808 - val_acc: 0.9039\n",
      "Epoch 27/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2776 - acc: 0.9040 - val_loss: 0.2674 - val_acc: 0.9075\n",
      "Epoch 28/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2762 - acc: 0.9034 - val_loss: 0.2822 - val_acc: 0.8979\n",
      "Epoch 29/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2775 - acc: 0.9041 - val_loss: 0.2710 - val_acc: 0.9026\n",
      "Epoch 30/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2771 - acc: 0.9037 - val_loss: 0.2675 - val_acc: 0.9104\n",
      "Epoch 31/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2749 - acc: 0.9048 - val_loss: 0.2699 - val_acc: 0.9041\n",
      "Epoch 32/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2756 - acc: 0.9040 - val_loss: 0.2629 - val_acc: 0.9073\n",
      "Epoch 33/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2750 - acc: 0.9047 - val_loss: 0.2787 - val_acc: 0.9041\n",
      "Epoch 34/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2738 - acc: 0.9044 - val_loss: 0.2683 - val_acc: 0.9033\n",
      "Epoch 35/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2731 - acc: 0.9046 - val_loss: 0.2625 - val_acc: 0.9091\n",
      "Epoch 36/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2726 - acc: 0.9049 - val_loss: 0.2641 - val_acc: 0.9077\n",
      "Epoch 37/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2727 - acc: 0.9047 - val_loss: 0.2626 - val_acc: 0.9104\n",
      "Epoch 38/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2724 - acc: 0.9039 - val_loss: 0.2597 - val_acc: 0.9089\n",
      "Epoch 39/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2713 - acc: 0.9048 - val_loss: 0.2644 - val_acc: 0.9099\n",
      "Epoch 40/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2707 - acc: 0.9049 - val_loss: 0.2641 - val_acc: 0.9063\n",
      "Epoch 41/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2699 - acc: 0.9059 - val_loss: 0.2659 - val_acc: 0.9061\n",
      "Epoch 42/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2696 - acc: 0.9060 - val_loss: 0.2743 - val_acc: 0.9115\n",
      "Epoch 43/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2683 - acc: 0.9062 - val_loss: 0.2636 - val_acc: 0.9060\n",
      "Epoch 44/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2686 - acc: 0.9064 - val_loss: 0.2588 - val_acc: 0.9091\n",
      "Epoch 45/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2690 - acc: 0.9055 - val_loss: 0.2588 - val_acc: 0.9083\n",
      "Epoch 46/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2680 - acc: 0.9056 - val_loss: 0.2693 - val_acc: 0.9106\n",
      "Epoch 47/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2665 - acc: 0.9062 - val_loss: 0.2590 - val_acc: 0.9079\n",
      "Epoch 48/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2672 - acc: 0.9066 - val_loss: 0.2611 - val_acc: 0.9089\n",
      "Epoch 49/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2682 - acc: 0.9063 - val_loss: 0.2562 - val_acc: 0.9095\n",
      "Epoch 50/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2667 - acc: 0.9064 - val_loss: 0.2602 - val_acc: 0.9116\n",
      "Epoch 51/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2666 - acc: 0.9064 - val_loss: 0.2609 - val_acc: 0.9074\n",
      "Epoch 52/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2657 - acc: 0.9068 - val_loss: 0.2566 - val_acc: 0.9090\n",
      "Epoch 53/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2661 - acc: 0.9072 - val_loss: 0.2606 - val_acc: 0.9062\n",
      "Epoch 54/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2660 - acc: 0.9058 - val_loss: 0.2629 - val_acc: 0.9019\n",
      "Epoch 55/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2651 - acc: 0.9071 - val_loss: 0.2556 - val_acc: 0.9077\n",
      "Epoch 56/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.2647 - acc: 0.9068 - val_loss: 0.2652 - val_acc: 0.9033\n",
      "Epoch 57/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2657 - acc: 0.9070 - val_loss: 0.2554 - val_acc: 0.9097\n",
      "Epoch 58/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2638 - acc: 0.9074 - val_loss: 0.2551 - val_acc: 0.9110\n",
      "Epoch 59/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2629 - acc: 0.9079 - val_loss: 0.2526 - val_acc: 0.9099\n",
      "Epoch 60/300\n",
      "49029/49029 [==============================] - 5s 102us/step - loss: 0.2630 - acc: 0.9075 - val_loss: 0.3213 - val_acc: 0.8689\n",
      "Epoch 61/300\n",
      "49029/49029 [==============================] - 7s 141us/step - loss: 0.2634 - acc: 0.9076 - val_loss: 0.2681 - val_acc: 0.8985\n",
      "Epoch 62/300\n",
      "49029/49029 [==============================] - 5s 109us/step - loss: 0.2633 - acc: 0.9072 - val_loss: 0.2569 - val_acc: 0.9085\n",
      "Epoch 63/300\n",
      "49029/49029 [==============================] - 6s 112us/step - loss: 0.2618 - acc: 0.9075 - val_loss: 0.2530 - val_acc: 0.9105\n",
      "Epoch 64/300\n",
      "49029/49029 [==============================] - 5s 94us/step - loss: 0.2626 - acc: 0.9070 - val_loss: 0.2542 - val_acc: 0.9088\n",
      "Epoch 65/300\n",
      "49029/49029 [==============================] - 6s 127us/step - loss: 0.2614 - acc: 0.9075 - val_loss: 0.2542 - val_acc: 0.9129\n",
      "Epoch 66/300\n",
      "49029/49029 [==============================] - 6s 121us/step - loss: 0.2613 - acc: 0.9080 - val_loss: 0.2633 - val_acc: 0.9040\n",
      "Epoch 67/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2606 - acc: 0.9074 - val_loss: 0.2536 - val_acc: 0.9086\n",
      "Epoch 68/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2620 - acc: 0.9079 - val_loss: 0.2501 - val_acc: 0.9091\n",
      "Epoch 69/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2594 - acc: 0.9088 - val_loss: 0.2512 - val_acc: 0.9100\n",
      "Epoch 70/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2606 - acc: 0.9077 - val_loss: 0.2533 - val_acc: 0.9079\n",
      "Epoch 71/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2591 - acc: 0.9083 - val_loss: 0.2780 - val_acc: 0.9026\n",
      "Epoch 72/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2599 - acc: 0.9085 - val_loss: 0.2495 - val_acc: 0.9101\n",
      "Epoch 73/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2589 - acc: 0.9081 - val_loss: 0.2496 - val_acc: 0.9101\n",
      "Epoch 74/300\n",
      "49029/49029 [==============================] - 4s 82us/step - loss: 0.2587 - acc: 0.9084 - val_loss: 0.2516 - val_acc: 0.9101\n",
      "Epoch 75/300\n",
      "49029/49029 [==============================] - 5s 103us/step - loss: 0.2605 - acc: 0.9072 - val_loss: 0.2508 - val_acc: 0.9108\n",
      "Epoch 76/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2594 - acc: 0.9082 - val_loss: 0.2527 - val_acc: 0.9113\n",
      "Epoch 77/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2578 - acc: 0.9087 - val_loss: 0.2545 - val_acc: 0.9050\n",
      "Epoch 78/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.2578 - acc: 0.9086 - val_loss: 0.2497 - val_acc: 0.9077\n",
      "Epoch 79/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2582 - acc: 0.9079 - val_loss: 0.2545 - val_acc: 0.9118\n",
      "Epoch 80/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2570 - acc: 0.9086 - val_loss: 0.2483 - val_acc: 0.9120\n",
      "Epoch 81/300\n",
      "49029/49029 [==============================] - 4s 76us/step - loss: 0.2564 - acc: 0.9095 - val_loss: 0.2486 - val_acc: 0.9121\n",
      "Epoch 82/300\n",
      "49029/49029 [==============================] - 4s 83us/step - loss: 0.2558 - acc: 0.9093 - val_loss: 0.2483 - val_acc: 0.9110\n",
      "Epoch 83/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2559 - acc: 0.9099 - val_loss: 0.2525 - val_acc: 0.9062\n",
      "Epoch 84/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2558 - acc: 0.9093 - val_loss: 0.2493 - val_acc: 0.9135\n",
      "Epoch 85/300\n",
      "49029/49029 [==============================] - 4s 87us/step - loss: 0.2569 - acc: 0.9094 - val_loss: 0.2470 - val_acc: 0.9109\n",
      "Epoch 86/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2560 - acc: 0.9087 - val_loss: 0.2498 - val_acc: 0.9142\n",
      "Epoch 87/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2557 - acc: 0.9101 - val_loss: 0.2521 - val_acc: 0.9147\n",
      "Epoch 88/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2555 - acc: 0.9096 - val_loss: 0.2530 - val_acc: 0.9100\n",
      "Epoch 89/300\n",
      "49029/49029 [==============================] - 4s 85us/step - loss: 0.2552 - acc: 0.9096 - val_loss: 0.2489 - val_acc: 0.9111 ETA: 0s - loss: 0.2559 -\n",
      "Epoch 90/300\n",
      "49029/49029 [==============================] - 4s 78us/step - loss: 0.2561 - acc: 0.9094 - val_loss: 0.2472 - val_acc: 0.9121\n",
      "Epoch 91/300\n",
      "49029/49029 [==============================] - 4s 89us/step - loss: 0.2551 - acc: 0.9105 - val_loss: 0.2464 - val_acc: 0.9112\n",
      "Epoch 92/300\n",
      "49029/49029 [==============================] - 6s 130us/step - loss: 0.2547 - acc: 0.9105 - val_loss: 0.2518 - val_acc: 0.9114\n",
      "Epoch 93/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2536 - acc: 0.9100 - val_loss: 0.2485 - val_acc: 0.9113\n",
      "Epoch 94/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.2538 - acc: 0.9103 - val_loss: 0.2489 - val_acc: 0.9144\n",
      "Epoch 95/300\n",
      "49029/49029 [==============================] - 4s 74us/step - loss: 0.2543 - acc: 0.9098 - val_loss: 0.2470 - val_acc: 0.9116\n",
      "Epoch 96/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2541 - acc: 0.9105 - val_loss: 0.2465 - val_acc: 0.9133\n",
      "Epoch 97/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2535 - acc: 0.9099 - val_loss: 0.2458 - val_acc: 0.9132\n",
      "Epoch 98/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2536 - acc: 0.9104 - val_loss: 0.2454 - val_acc: 0.9135\n",
      "Epoch 99/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2535 - acc: 0.9101 - val_loss: 0.2511 - val_acc: 0.9132\n",
      "Epoch 100/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2527 - acc: 0.9104 - val_loss: 0.2458 - val_acc: 0.9124\n",
      "Epoch 101/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2523 - acc: 0.9098 - val_loss: 0.2497 - val_acc: 0.9126\n",
      "Epoch 102/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.2529 - acc: 0.9100 - val_loss: 0.2429 - val_acc: 0.9128\n",
      "Epoch 103/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2528 - acc: 0.9102 - val_loss: 0.2483 - val_acc: 0.9132\n",
      "Epoch 104/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2528 - acc: 0.9108 - val_loss: 0.2458 - val_acc: 0.9130\n",
      "Epoch 105/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2521 - acc: 0.9113 - val_loss: 0.2486 - val_acc: 0.9109\n",
      "Epoch 106/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2524 - acc: 0.9107 - val_loss: 0.2507 - val_acc: 0.9114\n",
      "Epoch 107/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2522 - acc: 0.9108 - val_loss: 0.2456 - val_acc: 0.9120\n",
      "Epoch 108/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2516 - acc: 0.9105 - val_loss: 0.2433 - val_acc: 0.9125\n",
      "Epoch 109/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2517 - acc: 0.9115 - val_loss: 0.2464 - val_acc: 0.9120\n",
      "Epoch 110/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2515 - acc: 0.9103 - val_loss: 0.2438 - val_acc: 0.9131\n",
      "Epoch 111/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2505 - acc: 0.9110 - val_loss: 0.2459 - val_acc: 0.9123\n",
      "Epoch 112/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2513 - acc: 0.9115 - val_loss: 0.2501 - val_acc: 0.9136\n",
      "Epoch 113/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2509 - acc: 0.9109 - val_loss: 0.2475 - val_acc: 0.9134\n",
      "Epoch 114/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2515 - acc: 0.9115 - val_loss: 0.2502 - val_acc: 0.9106\n",
      "Epoch 115/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2516 - acc: 0.9103 - val_loss: 0.2452 - val_acc: 0.9150\n",
      "Epoch 116/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2511 - acc: 0.9108 - val_loss: 0.2446 - val_acc: 0.9130\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2511 - acc: 0.9113 - val_loss: 0.2455 - val_acc: 0.9107\n",
      "Epoch 118/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2503 - acc: 0.9114 - val_loss: 0.2432 - val_acc: 0.9129\n",
      "Epoch 119/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2500 - acc: 0.9112 - val_loss: 0.2558 - val_acc: 0.9130\n",
      "Epoch 120/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2511 - acc: 0.9110 - val_loss: 0.2449 - val_acc: 0.9139\n",
      "Epoch 121/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2500 - acc: 0.9120 - val_loss: 0.2487 - val_acc: 0.9121\n",
      "Epoch 122/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2496 - acc: 0.9112 - val_loss: 0.2400 - val_acc: 0.9128\n",
      "Epoch 123/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2504 - acc: 0.9111 - val_loss: 0.2436 - val_acc: 0.9125\n",
      "Epoch 124/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2502 - acc: 0.9110 - val_loss: 0.2441 - val_acc: 0.9134\n",
      "Epoch 125/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2503 - acc: 0.9115 - val_loss: 0.2450 - val_acc: 0.9135\n",
      "Epoch 126/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2496 - acc: 0.9114 - val_loss: 0.2422 - val_acc: 0.9106\n",
      "Epoch 127/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2490 - acc: 0.9110 - val_loss: 0.2546 - val_acc: 0.9068\n",
      "Epoch 128/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2494 - acc: 0.9120 - val_loss: 0.2414 - val_acc: 0.9134\n",
      "Epoch 129/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2479 - acc: 0.9117 - val_loss: 0.2466 - val_acc: 0.9140\n",
      "Epoch 130/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2490 - acc: 0.9120 - val_loss: 0.2438 - val_acc: 0.9137\n",
      "Epoch 131/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2493 - acc: 0.9121 - val_loss: 0.2459 - val_acc: 0.9132\n",
      "Epoch 132/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2496 - acc: 0.9111 - val_loss: 0.2428 - val_acc: 0.9132\n",
      "Epoch 133/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2492 - acc: 0.9113 - val_loss: 0.2459 - val_acc: 0.9130\n",
      "Epoch 134/300\n",
      "49029/49029 [==============================] - 3s 68us/step - loss: 0.2484 - acc: 0.9113 - val_loss: 0.2422 - val_acc: 0.9115\n",
      "Epoch 135/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2484 - acc: 0.9116 - val_loss: 0.2434 - val_acc: 0.9110\n",
      "Epoch 136/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2475 - acc: 0.9119 - val_loss: 0.2432 - val_acc: 0.9101\n",
      "Epoch 137/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2486 - acc: 0.9117 - val_loss: 0.2459 - val_acc: 0.9129\n",
      "Epoch 138/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2486 - acc: 0.9114 - val_loss: 0.2414 - val_acc: 0.9119\n",
      "Epoch 139/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2481 - acc: 0.9123 - val_loss: 0.2417 - val_acc: 0.9132\n",
      "Epoch 140/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2482 - acc: 0.9120 - val_loss: 0.2406 - val_acc: 0.9134\n",
      "Epoch 141/300\n",
      "49029/49029 [==============================] - 3s 66us/step - loss: 0.2482 - acc: 0.9117 - val_loss: 0.2396 - val_acc: 0.9147\n",
      "Epoch 142/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2467 - acc: 0.9122 - val_loss: 0.2402 - val_acc: 0.9138\n",
      "Epoch 143/300\n",
      "49029/49029 [==============================] - 3s 68us/step - loss: 0.2474 - acc: 0.9119 - val_loss: 0.2471 - val_acc: 0.9122\n",
      "Epoch 144/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2476 - acc: 0.9116 - val_loss: 0.2435 - val_acc: 0.9134\n",
      "Epoch 145/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2464 - acc: 0.9116 - val_loss: 0.2403 - val_acc: 0.9139\n",
      "Epoch 146/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2478 - acc: 0.9118 - val_loss: 0.2413 - val_acc: 0.9136\n",
      "Epoch 147/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2458 - acc: 0.9123 - val_loss: 0.2426 - val_acc: 0.9139\n",
      "Epoch 148/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2475 - acc: 0.9116 - val_loss: 0.2420 - val_acc: 0.9145\n",
      "Epoch 149/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2462 - acc: 0.9123 - val_loss: 0.2429 - val_acc: 0.9144\n",
      "Epoch 150/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2462 - acc: 0.9115 - val_loss: 0.2456 - val_acc: 0.9110\n",
      "Epoch 151/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2466 - acc: 0.9119 - val_loss: 0.2396 - val_acc: 0.9144\n",
      "Epoch 152/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2461 - acc: 0.9121 - val_loss: 0.2378 - val_acc: 0.9154\n",
      "Epoch 153/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2459 - acc: 0.9124 - val_loss: 0.2401 - val_acc: 0.9158\n",
      "Epoch 154/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2461 - acc: 0.9127 - val_loss: 0.2438 - val_acc: 0.9159\n",
      "Epoch 155/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2462 - acc: 0.9121 - val_loss: 0.2434 - val_acc: 0.9119\n",
      "Epoch 156/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2461 - acc: 0.9120 - val_loss: 0.2440 - val_acc: 0.9115\n",
      "Epoch 157/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2464 - acc: 0.9120 - val_loss: 0.2414 - val_acc: 0.9134\n",
      "Epoch 158/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2453 - acc: 0.9124 - val_loss: 0.2392 - val_acc: 0.9116\n",
      "Epoch 159/300\n",
      "49029/49029 [==============================] - 4s 77us/step - loss: 0.2459 - acc: 0.9121 - val_loss: 0.2414 - val_acc: 0.9132\n",
      "Epoch 160/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.2456 - acc: 0.9123 - val_loss: 0.2510 - val_acc: 0.9108\n",
      "Epoch 161/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2450 - acc: 0.9124 - val_loss: 0.2456 - val_acc: 0.9113\n",
      "Epoch 162/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2456 - acc: 0.9123 - val_loss: 0.2428 - val_acc: 0.9129\n",
      "Epoch 163/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2456 - acc: 0.9126 - val_loss: 0.2382 - val_acc: 0.9133\n",
      "Epoch 164/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2448 - acc: 0.9128 - val_loss: 0.2461 - val_acc: 0.9146\n",
      "Epoch 165/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2454 - acc: 0.9120 - val_loss: 0.2411 - val_acc: 0.9166\n",
      "Epoch 166/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2438 - acc: 0.9127 - val_loss: 0.2407 - val_acc: 0.9130\n",
      "Epoch 167/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2450 - acc: 0.9126 - val_loss: 0.2438 - val_acc: 0.9155\n",
      "Epoch 168/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2441 - acc: 0.9123 - val_loss: 0.2383 - val_acc: 0.9114\n",
      "Epoch 169/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2448 - acc: 0.9124 - val_loss: 0.2430 - val_acc: 0.9154\n",
      "Epoch 170/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2448 - acc: 0.9123 - val_loss: 0.2388 - val_acc: 0.9137\n",
      "Epoch 171/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2441 - acc: 0.9126 - val_loss: 0.2458 - val_acc: 0.9019\n",
      "Epoch 172/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2448 - acc: 0.9130 - val_loss: 0.2418 - val_acc: 0.9126\n",
      "Epoch 173/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2451 - acc: 0.9119 - val_loss: 0.2377 - val_acc: 0.9166\n",
      "Epoch 174/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2438 - acc: 0.9125 - val_loss: 0.2392 - val_acc: 0.9148\n",
      "Epoch 175/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2438 - acc: 0.9131 - val_loss: 0.2405 - val_acc: 0.9144\n",
      "Epoch 176/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2457 - acc: 0.9118 - val_loss: 0.2401 - val_acc: 0.9154\n",
      "Epoch 177/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2437 - acc: 0.9129 - val_loss: 0.2400 - val_acc: 0.9148\n",
      "Epoch 178/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2447 - acc: 0.9130 - val_loss: 0.2406 - val_acc: 0.9145\n",
      "Epoch 179/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2447 - acc: 0.9121 - val_loss: 0.2409 - val_acc: 0.9101\n",
      "Epoch 180/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2436 - acc: 0.9132 - val_loss: 0.2354 - val_acc: 0.9159\n",
      "Epoch 181/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2435 - acc: 0.9127 - val_loss: 0.2376 - val_acc: 0.9148\n",
      "Epoch 182/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2441 - acc: 0.9120 - val_loss: 0.2417 - val_acc: 0.9133\n",
      "Epoch 183/300\n",
      "49029/49029 [==============================] - 3s 68us/step - loss: 0.2429 - acc: 0.9132 - val_loss: 0.2371 - val_acc: 0.9185\n",
      "Epoch 184/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2428 - acc: 0.9130 - val_loss: 0.2395 - val_acc: 0.9157\n",
      "Epoch 185/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2479 - acc: 0.9124 - val_loss: 0.2557 - val_acc: 0.9102\n",
      "Epoch 186/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2437 - acc: 0.9131 - val_loss: 0.2430 - val_acc: 0.9136\n",
      "Epoch 187/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2423 - acc: 0.9133 - val_loss: 0.2359 - val_acc: 0.9150\n",
      "Epoch 188/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2430 - acc: 0.9137 - val_loss: 0.2419 - val_acc: 0.9119\n",
      "Epoch 189/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2435 - acc: 0.9127 - val_loss: 0.2418 - val_acc: 0.9140\n",
      "Epoch 190/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2428 - acc: 0.9141 - val_loss: 0.2391 - val_acc: 0.9181\n",
      "Epoch 191/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2438 - acc: 0.9144 - val_loss: 0.2423 - val_acc: 0.9146\n",
      "Epoch 192/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2427 - acc: 0.9136 - val_loss: 0.2366 - val_acc: 0.9188\n",
      "Epoch 193/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2423 - acc: 0.9134 - val_loss: 0.2360 - val_acc: 0.9174\n",
      "Epoch 194/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2427 - acc: 0.9139 - val_loss: 0.2395 - val_acc: 0.9151\n",
      "Epoch 195/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2424 - acc: 0.9139 - val_loss: 0.2391 - val_acc: 0.9126\n",
      "Epoch 196/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2421 - acc: 0.9146 - val_loss: 0.2411 - val_acc: 0.9160\n",
      "Epoch 197/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2410 - acc: 0.9140 - val_loss: 0.2394 - val_acc: 0.9151\n",
      "Epoch 198/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2436 - acc: 0.9147 - val_loss: 0.2411 - val_acc: 0.9153\n",
      "Epoch 199/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2421 - acc: 0.9145 - val_loss: 0.2363 - val_acc: 0.9132\n",
      "Epoch 200/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2432 - acc: 0.9140 - val_loss: 0.2363 - val_acc: 0.9146\n",
      "Epoch 201/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2429 - acc: 0.9137 - val_loss: 0.2393 - val_acc: 0.9126\n",
      "Epoch 202/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2421 - acc: 0.9147 - val_loss: 0.2403 - val_acc: 0.9131\n",
      "Epoch 203/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2410 - acc: 0.9138 - val_loss: 0.2386 - val_acc: 0.9167\n",
      "Epoch 204/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2417 - acc: 0.9143 - val_loss: 0.2462 - val_acc: 0.9140\n",
      "Epoch 205/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2415 - acc: 0.9139 - val_loss: 0.2378 - val_acc: 0.9138\n",
      "Epoch 206/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2413 - acc: 0.9139 - val_loss: 0.2364 - val_acc: 0.9174\n",
      "Epoch 207/300\n",
      "49029/49029 [==============================] - 3s 68us/step - loss: 0.2419 - acc: 0.9139 - val_loss: 0.2411 - val_acc: 0.9116\n",
      "Epoch 208/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2414 - acc: 0.9148 - val_loss: 0.2393 - val_acc: 0.9160\n",
      "Epoch 209/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2417 - acc: 0.9146 - val_loss: 0.2373 - val_acc: 0.9162\n",
      "Epoch 210/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2415 - acc: 0.9147 - val_loss: 0.2377 - val_acc: 0.9156\n",
      "Epoch 211/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2410 - acc: 0.9142 - val_loss: 0.2369 - val_acc: 0.9154\n",
      "Epoch 212/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2414 - acc: 0.9144 - val_loss: 0.2441 - val_acc: 0.9118\n",
      "Epoch 213/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2419 - acc: 0.9144 - val_loss: 0.2384 - val_acc: 0.9153\n",
      "Epoch 214/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2425 - acc: 0.9140 - val_loss: 0.2420 - val_acc: 0.9148\n",
      "Epoch 215/300\n",
      "49029/49029 [==============================] - 4s 75us/step - loss: 0.2411 - acc: 0.9146 - val_loss: 0.2436 - val_acc: 0.9135\n",
      "Epoch 216/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2408 - acc: 0.9150 - val_loss: 0.2372 - val_acc: 0.9157\n",
      "Epoch 217/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2410 - acc: 0.9158 - val_loss: 0.2398 - val_acc: 0.9165\n",
      "Epoch 218/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2411 - acc: 0.9143 - val_loss: 0.2512 - val_acc: 0.9143\n",
      "Epoch 219/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2405 - acc: 0.9146 - val_loss: 0.2348 - val_acc: 0.9190\n",
      "Epoch 220/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2408 - acc: 0.9142 - val_loss: 0.2362 - val_acc: 0.9175\n",
      "Epoch 221/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2409 - acc: 0.9144 - val_loss: 0.2351 - val_acc: 0.9177\n",
      "Epoch 222/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2404 - acc: 0.9151 - val_loss: 0.2373 - val_acc: 0.9146\n",
      "Epoch 223/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2404 - acc: 0.9148 - val_loss: 0.2356 - val_acc: 0.9172\n",
      "Epoch 224/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2408 - acc: 0.9154 - val_loss: 0.2386 - val_acc: 0.9144\n",
      "Epoch 225/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2402 - acc: 0.9151 - val_loss: 0.2346 - val_acc: 0.9195\n",
      "Epoch 226/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2409 - acc: 0.9148 - val_loss: 0.2350 - val_acc: 0.9188\n",
      "Epoch 227/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2407 - acc: 0.9149 - val_loss: 0.2335 - val_acc: 0.9193\n",
      "Epoch 228/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2402 - acc: 0.9151 - val_loss: 0.2414 - val_acc: 0.9155\n",
      "Epoch 229/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2397 - acc: 0.9156 - val_loss: 0.2654 - val_acc: 0.9102\n",
      "Epoch 230/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2408 - acc: 0.9152 - val_loss: 0.2338 - val_acc: 0.9176\n",
      "Epoch 231/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2396 - acc: 0.9151 - val_loss: 0.2382 - val_acc: 0.9149\n",
      "Epoch 232/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2401 - acc: 0.9155 - val_loss: 0.2358 - val_acc: 0.9163\n",
      "Epoch 233/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2401 - acc: 0.9153 - val_loss: 0.2341 - val_acc: 0.9169\n",
      "Epoch 234/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2403 - acc: 0.9150 - val_loss: 0.2367 - val_acc: 0.9188\n",
      "Epoch 235/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2397 - acc: 0.9152 - val_loss: 0.2363 - val_acc: 0.9167\n",
      "Epoch 236/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2402 - acc: 0.9155 - val_loss: 0.2341 - val_acc: 0.9176\n",
      "Epoch 237/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2410 - acc: 0.9143 - val_loss: 0.2360 - val_acc: 0.9165\n",
      "Epoch 238/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2396 - acc: 0.9154 - val_loss: 0.2367 - val_acc: 0.9160\n",
      "Epoch 239/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2398 - acc: 0.9152 - val_loss: 0.2356 - val_acc: 0.9189\n",
      "Epoch 240/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2396 - acc: 0.9150 - val_loss: 0.2404 - val_acc: 0.9129\n",
      "Epoch 241/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2400 - acc: 0.9148 - val_loss: 0.2360 - val_acc: 0.9125\n",
      "Epoch 242/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2401 - acc: 0.9153 - val_loss: 0.2382 - val_acc: 0.9177\n",
      "Epoch 243/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2388 - acc: 0.9157 - val_loss: 0.2320 - val_acc: 0.9181\n",
      "Epoch 244/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2398 - acc: 0.9148 - val_loss: 0.2438 - val_acc: 0.9188\n",
      "Epoch 245/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2413 - acc: 0.9145 - val_loss: 0.2344 - val_acc: 0.9188\n",
      "Epoch 246/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2396 - acc: 0.9155 - val_loss: 0.2346 - val_acc: 0.9152\n",
      "Epoch 247/300\n",
      "49029/49029 [==============================] - 4s 79us/step - loss: 0.2383 - acc: 0.9166 - val_loss: 0.2350 - val_acc: 0.9172\n",
      "Epoch 248/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2389 - acc: 0.9157 - val_loss: 0.2372 - val_acc: 0.9176\n",
      "Epoch 249/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2390 - acc: 0.9160 - val_loss: 0.2334 - val_acc: 0.9186\n",
      "Epoch 250/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2402 - acc: 0.9155 - val_loss: 0.2403 - val_acc: 0.9138\n",
      "Epoch 251/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2386 - acc: 0.9159 - val_loss: 0.2378 - val_acc: 0.9134\n",
      "Epoch 252/300\n",
      "49029/49029 [==============================] - 3s 68us/step - loss: 0.2383 - acc: 0.9154 - val_loss: 0.2361 - val_acc: 0.9151\n",
      "Epoch 253/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2388 - acc: 0.9167 - val_loss: 0.2359 - val_acc: 0.9170\n",
      "Epoch 254/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2388 - acc: 0.9157 - val_loss: 0.2335 - val_acc: 0.9190\n",
      "Epoch 255/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2388 - acc: 0.9163 - val_loss: 0.2373 - val_acc: 0.9133\n",
      "Epoch 256/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2387 - acc: 0.9159 - val_loss: 0.2386 - val_acc: 0.9155\n",
      "Epoch 257/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2381 - acc: 0.9155 - val_loss: 0.2336 - val_acc: 0.9190\n",
      "Epoch 258/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2385 - acc: 0.9156 - val_loss: 0.2354 - val_acc: 0.9186\n",
      "Epoch 259/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2388 - acc: 0.9158 - val_loss: 0.2388 - val_acc: 0.9104\n",
      "Epoch 260/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2383 - acc: 0.9153 - val_loss: 0.2393 - val_acc: 0.9130\n",
      "Epoch 261/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2368 - acc: 0.9156 - val_loss: 0.2329 - val_acc: 0.9184\n",
      "Epoch 262/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2373 - acc: 0.9156 - val_loss: 0.2353 - val_acc: 0.9165\n",
      "Epoch 263/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2374 - acc: 0.9159 - val_loss: 0.2356 - val_acc: 0.9176\n",
      "Epoch 264/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2367 - acc: 0.9162 - val_loss: 0.2399 - val_acc: 0.9140\n",
      "Epoch 265/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2372 - acc: 0.9158 - val_loss: 0.2378 - val_acc: 0.9137\n",
      "Epoch 266/300\n",
      "49029/49029 [==============================] - 3s 71us/step - loss: 0.2381 - acc: 0.9154 - val_loss: 0.2333 - val_acc: 0.9201\n",
      "Epoch 267/300\n",
      "49029/49029 [==============================] - 4s 72us/step - loss: 0.2370 - acc: 0.9157 - val_loss: 0.2336 - val_acc: 0.9194\n",
      "Epoch 268/300\n",
      "49029/49029 [==============================] - 4s 73us/step - loss: 0.2362 - acc: 0.9159 - val_loss: 0.2340 - val_acc: 0.9188\n",
      "Epoch 269/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2363 - acc: 0.9154 - val_loss: 0.2361 - val_acc: 0.9164\n",
      "Epoch 270/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2358 - acc: 0.9163 - val_loss: 0.2324 - val_acc: 0.9188\n",
      "Epoch 271/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2352 - acc: 0.9167 - val_loss: 0.2328 - val_acc: 0.9174\n",
      "Epoch 272/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2352 - acc: 0.9161 - val_loss: 0.2332 - val_acc: 0.9189\n",
      "Epoch 273/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2359 - acc: 0.9160 - val_loss: 0.2331 - val_acc: 0.9180\n",
      "Epoch 274/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2354 - acc: 0.9158 - val_loss: 0.2349 - val_acc: 0.9182\n",
      "Epoch 275/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2355 - acc: 0.9164 - val_loss: 0.2319 - val_acc: 0.9178\n",
      "Epoch 276/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2350 - acc: 0.9168 - val_loss: 0.2399 - val_acc: 0.9127\n",
      "Epoch 277/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2356 - acc: 0.9156 - val_loss: 0.2366 - val_acc: 0.9158\n",
      "Epoch 278/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2355 - acc: 0.9162 - val_loss: 0.2329 - val_acc: 0.9159\n",
      "Epoch 279/300\n",
      "49029/49029 [==============================] - 3s 68us/step - loss: 0.2355 - acc: 0.9157 - val_loss: 0.2338 - val_acc: 0.9161\n",
      "Epoch 280/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2357 - acc: 0.9160 - val_loss: 0.2295 - val_acc: 0.9185\n",
      "Epoch 281/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2342 - acc: 0.9170 - val_loss: 0.2365 - val_acc: 0.9149\n",
      "Epoch 282/300\n",
      "49029/49029 [==============================] - 3s 70us/step - loss: 0.2347 - acc: 0.9160 - val_loss: 0.2312 - val_acc: 0.9173\n",
      "Epoch 283/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2356 - acc: 0.9170 - val_loss: 0.2321 - val_acc: 0.9161\n",
      "Epoch 284/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2351 - acc: 0.9160 - val_loss: 0.2307 - val_acc: 0.9182\n",
      "Epoch 285/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2351 - acc: 0.9155 - val_loss: 0.2307 - val_acc: 0.9202\n",
      "Epoch 286/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2348 - acc: 0.9164 - val_loss: 0.2312 - val_acc: 0.9187\n",
      "Epoch 287/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2342 - acc: 0.9164 - val_loss: 0.2343 - val_acc: 0.9185\n",
      "Epoch 288/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2349 - acc: 0.9167 - val_loss: 0.2314 - val_acc: 0.9200\n",
      "Epoch 289/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2354 - acc: 0.9167 - val_loss: 0.2358 - val_acc: 0.9150\n",
      "Epoch 290/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2367 - acc: 0.9158 - val_loss: 0.2338 - val_acc: 0.9165\n",
      "Epoch 291/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2369 - acc: 0.9160 - val_loss: 0.2297 - val_acc: 0.9195\n",
      "Epoch 292/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2347 - acc: 0.9161 - val_loss: 0.2301 - val_acc: 0.9183\n",
      "Epoch 293/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2334 - acc: 0.9164 - val_loss: 0.2311 - val_acc: 0.9189\n",
      "Epoch 294/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2338 - acc: 0.9168 - val_loss: 0.2315 - val_acc: 0.9185\n",
      "Epoch 295/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2347 - acc: 0.9163 - val_loss: 0.2336 - val_acc: 0.9161\n",
      "Epoch 296/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2343 - acc: 0.9166 - val_loss: 0.2382 - val_acc: 0.9152\n",
      "Epoch 297/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2335 - acc: 0.9169 - val_loss: 0.2317 - val_acc: 0.9188\n",
      "Epoch 298/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2338 - acc: 0.9167 - val_loss: 0.2308 - val_acc: 0.9191\n",
      "Epoch 299/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2339 - acc: 0.9166 - val_loss: 0.2298 - val_acc: 0.9200\n",
      "Epoch 300/300\n",
      "49029/49029 [==============================] - 3s 69us/step - loss: 0.2336 - acc: 0.9174 - val_loss: 0.2302 - val_acc: 0.9192\n",
      "73178/73178 [==============================] - 1s 20us/step\n",
      "18295/18295 [==============================] - 0s 21us/step\n",
      "Training set accuracy: 0.9186914099882371\n",
      "Training set loss: 0.23039320319912968\n",
      "Test set accuracy: 0.9189396010099392\n",
      "Test set loss: 0.22892171074846268\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#60,30,1 --> .9156 acc with .234 loss\n",
    "#20,10,1 --> .9122 acc, .2576 loss\n",
    "#10,5,1 --> val: .9064 acc, .279 loss\n",
    "#30,10,1 --> .9117 acc, .239 loss\n",
    "#60,10,1 --> .918 acc , .230 loss\n",
    "#70,10,1 --> .913, .238\n",
    "#50,10,1 --> .917 , .229\n",
    "#40,10,1 --> .918, .228\n",
    "model = Sequential()\n",
    "model.add(Dense(40, activation='sigmoid', input_dim = 4))\n",
    "model.add(Dense(10,activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# can try rmsprop\n",
    "model.compile(optimizer = 'adam',     \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy']) \n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_split = 0.33, \n",
    "                    epochs = 300, \n",
    "                    batch_size = 32)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "print('Training set accuracy:', train_acc)\n",
    "print('Training set loss:', train_loss)\n",
    "\n",
    "print('Test set accuracy:', test_acc)\n",
    "print('Test set loss:', test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The history of our accuracy during training.\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The history of our cross-entropy loss during training.\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting to Kaggle with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "true_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "#also remember to pickle model for kyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test_X = true_test[['app', 'device', 'os', 'channel']]\n",
    "\n",
    "true_test_predictions = model.predict(true_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-a888ac5be415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m submission = pd.DataFrame({\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0;34m\"click_id,is_attributed\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrue_test_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     }).to_csv('prediction.csv')\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# saves to prediction.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             raise ValueError('If using all scalar values, you must pass'\n\u001b[0m\u001b[1;32m    309\u001b[0m                              ' an index')\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"click_id,is_attributed\": true_test_predictions\n",
    "    }).to_csv('prediction.csv')\n",
    "# saves to prediction.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
